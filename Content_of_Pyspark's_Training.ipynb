{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakshiVarma72/pySparkCode/blob/Assignments_Codes/Content_of_Pyspark's_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZHqeKJz8QWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de67a7fe-d11f-44fb-86c7-64e532747557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Waiting for headers] [1 I\r0% [Connecting to archive.ubuntu.com (91.189.91.38)] [Waiting for headers] [Con\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Packages [81.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,079 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,636 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease [24.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [39.5 kB]\n",
            "Hit:15 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,604 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,229 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal/main amd64 Packages [44.1 kB]\n",
            "Fetched 8,096 kB in 5s (1,788 kB/s)\n",
            "Reading package lists... Done\n",
            "spark-3.3.2-bin-hadoop3/\n",
            "spark-3.3.2-bin-hadoop3/LICENSE\n",
            "spark-3.3.2-bin-hadoop3/NOTICE\n",
            "spark-3.3.2-bin-hadoop3/R/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/INDEX\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/NAMESPACE\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/R/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/R/SparkR\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/doc/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/doc/index.html\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/AnIndex\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/help/paths.rds\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/html/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/html/00Index.html\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/html/R.css\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/profile/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/profile/general.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/profile/shell.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/tests/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/tests/testthat/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/worker/\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/SparkR/worker/worker.R\n",
            "spark-3.3.2-bin-hadoop3/R/lib/sparkr.zip\n",
            "spark-3.3.2-bin-hadoop3/README.md\n",
            "spark-3.3.2-bin-hadoop3/RELEASE\n",
            "spark-3.3.2-bin-hadoop3/bin/\n",
            "spark-3.3.2-bin-hadoop3/bin/beeline\n",
            "spark-3.3.2-bin-hadoop3/bin/beeline.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/docker-image-tool.sh\n",
            "spark-3.3.2-bin-hadoop3/bin/find-spark-home\n",
            "spark-3.3.2-bin-hadoop3/bin/find-spark-home.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/load-spark-env.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/load-spark-env.sh\n",
            "spark-3.3.2-bin-hadoop3/bin/pyspark\n",
            "spark-3.3.2-bin-hadoop3/bin/pyspark.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/pyspark2.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/run-example\n",
            "spark-3.3.2-bin-hadoop3/bin/run-example.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-class\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-class.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-class2.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-shell\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-shell.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-shell2.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-sql\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-sql.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-sql2.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-submit\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-submit.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/spark-submit2.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/sparkR\n",
            "spark-3.3.2-bin-hadoop3/bin/sparkR.cmd\n",
            "spark-3.3.2-bin-hadoop3/bin/sparkR2.cmd\n",
            "spark-3.3.2-bin-hadoop3/conf/\n",
            "spark-3.3.2-bin-hadoop3/conf/fairscheduler.xml.template\n",
            "spark-3.3.2-bin-hadoop3/conf/log4j2.properties.template\n",
            "spark-3.3.2-bin-hadoop3/conf/metrics.properties.template\n",
            "spark-3.3.2-bin-hadoop3/conf/spark-defaults.conf.template\n",
            "spark-3.3.2-bin-hadoop3/conf/spark-env.sh.template\n",
            "spark-3.3.2-bin-hadoop3/conf/workers.template\n",
            "spark-3.3.2-bin-hadoop3/data/\n",
            "spark-3.3.2-bin-hadoop3/data/graphx/\n",
            "spark-3.3.2-bin-hadoop3/data/graphx/followers.txt\n",
            "spark-3.3.2-bin-hadoop3/data/graphx/users.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/als/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/als/test.data\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/gmm_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/license.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/license.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/multi-channel/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/kmeans_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/pagerank_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/pic_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/ridge-data/\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_lda_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_movielens_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/sample_svm_data.txt\n",
            "spark-3.3.2-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.3.2-bin-hadoop3/data/streaming/\n",
            "spark-3.3.2-bin-hadoop3/data/streaming/AFINN-111.txt\n",
            "spark-3.3.2-bin-hadoop3/examples/\n",
            "spark-3.3.2-bin-hadoop3/examples/jars/\n",
            "spark-3.3.2-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.3.2-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/examples/src/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/als.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/kmeans.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/logistic_regression.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/__init__,py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/als_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/pagerank.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/pi.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sort.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/arrow.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/basic.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/datasource.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/hive.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/__init__,py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/sql/streaming/structured_sessionization.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/status_api_demo.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/transitive_closure.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/python/wordcount.py\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/data-manipulation.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/dataframe.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/als.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/fpm.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/gbt.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/glm.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/kstest.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/lda.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/logit.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/ml.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/mlp.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/survreg.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/META-INF/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/META-INF/services/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/dir1/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/employees.json\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/full_user.avsc\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/kv1.txt\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/people.csv\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/people.json\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/people.txt\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/user.avsc\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/users.avro\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/users.orc\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/resources/users.parquet\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MiniReadWriteTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scripts/\n",
            "spark-3.3.2-bin-hadoop3/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.3.2-bin-hadoop3/jars/\n",
            "spark-3.3.2-bin-hadoop3/jars/HikariCP-2.5.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/JLargeArrays-1.5.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/JTransforms-3.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/RoaringBitmap-0.9.25.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/ST4-4.0.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/activation-1.1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/aircompressor-0.21.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/algebra_2.12-2.0.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/annotations-17.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/antlr4-runtime-4.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arpack-2.2.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arrow-format-7.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arrow-memory-core-7.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arrow-memory-netty-7.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/arrow-vector-7.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/automaton-1.11-8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/avro-1.11.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/avro-ipc-1.11.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/avro-mapred-1.11.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/blas-2.2.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/breeze-macros_2.12-1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/breeze_2.12-1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/chill-java-0.10.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/chill_2.12-0.10.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-cli-1.5.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-codec-1.15.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-collections-3.2.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-collections4-4.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-compress-1.21.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-dbcp-1.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-io-2.11.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-lang-2.6.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-lang3-3.12.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-logging-1.1.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-math3-3.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-pool-1.5.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/commons-text-1.10.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/compress-lzf-1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/core-1.1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/curator-client-2.13.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/curator-framework-2.13.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/derby-10.14.2.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/generex-1.0.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/gson-2.2.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/guava-14.0.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hadoop-client-api-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hadoop-client-runtime-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-beeline-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-cli-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-common-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-exec-2.3.9-core.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-jdbc-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-llap-common-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-metastore-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-serde-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-shims-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-shims-common-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hive-vector-code-gen-2.3.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hk2-api-2.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/httpclient-4.5.13.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/httpcore-4.4.14.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-annotations-2.13.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-core-2.13.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-databind-2.13.4.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-dataformat-yaml-2.13.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-datatype-jsr310-2.13.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jackson-module-scala_2.12-2.13.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/janino-3.0.16.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/javolution-5.5.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jcl-over-slf4j-1.7.32.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jdo-api-3.0.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-client-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-common-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-container-servlet-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-container-servlet-core-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-hk2-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jersey-server-2.36.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jline-2.14.6.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/joda-time-2.10.13.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jodd-core-3.5.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jpam-1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/json-1.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jsr305-3.0.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jta-1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/jul-to-slf4j-1.7.32.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-client-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-admissionregistration-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-apiextensions-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-apps-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-autoscaling-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-batch-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-certificates-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-common-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-coordination-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-core-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-discovery-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-events-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-extensions-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-flowcontrol-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-metrics-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-networking-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-node-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-policy-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-rbac-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-scheduling-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/kubernetes-model-storageclass-5.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/lapack-2.2.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/libfb303-0.9.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/libthrift-0.12.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/log4j-1.2-api-2.17.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/log4j-api-2.17.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/log4j-core-2.17.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/log4j-slf4j-impl-2.17.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/lz4-java-1.8.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/metrics-core-4.2.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/metrics-graphite-4.2.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/metrics-jmx-4.2.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/metrics-json-4.2.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/metrics-jvm-4.2.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/minlog-1.3.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-all-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-buffer-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-codec-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-common-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-handler-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-resolver-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-tcnative-classes-2.0.48.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.74.Final.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/objenesis-3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/okhttp-3.12.12.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/okio-1.14.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/opencsv-2.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/orc-core-1.7.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/orc-mapreduce-1.7.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/orc-shims-1.7.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/oro-2.0.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/paranamer-2.8.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-column-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-common-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-encoding-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-format-structures-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-hadoop-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/parquet-jackson-1.12.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/pickle-1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/py4j-0.10.9.5.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/rocksdbjni-6.20.3.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-compiler-2.12.15.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-library-2.12.15.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-reflect-2.12.15.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/shapeless_2.12-2.3.7.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/shims-0.9.25.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/slf4j-api-1.7.32.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/snakeyaml-1.31.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/snappy-java-1.1.8.4.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-catalyst_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-core_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-graphx_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-hive_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-kubernetes_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-kvstore_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-launcher_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-mesos_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-mllib-local_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-mllib_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-network-common_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-network-shuffle_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-repl_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-sketch_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-sql_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-streaming_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-tags_2.12-3.3.2-tests.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-tags_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-unsafe_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spark-yarn_2.12-3.3.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/spire_2.12-0.17.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/stax-api-1.0.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/stream-2.9.6.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/super-csv-2.2.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/tink-1.6.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/transaction-api-1.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/velocity-1.5.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/xbean-asm9-shaded-4.20.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/xz-1.9.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/zookeeper-3.6.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/zookeeper-jute-3.6.2.jar\n",
            "spark-3.3.2-bin-hadoop3/jars/zstd-jni-1.5.2-1.jar\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile.java17\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/autoscale.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/decommissioning.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/py_container_checks.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/pyfiles.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/python_executable_check.py\n",
            "spark-3.3.2-bin-hadoop3/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.3.2-bin-hadoop3/licenses/\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-CC0.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-antlr.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-arpack.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-automaton.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-blas.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-datatables.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-f2j.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-janino.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-javassist.html\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-javolution.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jline.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jodd.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-join.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jquery.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-kryo.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-machinist.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-minlog.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-modernizr.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-mustache.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-netlib.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-paranamer.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-protobuf.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-py4j.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-re2j.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-respond.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-scala.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-scopt.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-slf4j.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-spire.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.3.2-bin-hadoop3/licenses/LICENSE-zstd.txt\n",
            "spark-3.3.2-bin-hadoop3/python/\n",
            "spark-3.3.2-bin-hadoop3/python/.coveragerc\n",
            "spark-3.3.2-bin-hadoop3/python/.gitignore\n",
            "spark-3.3.2-bin-hadoop3/python/MANIFEST.in\n",
            "spark-3.3.2-bin-hadoop3/python/README.md\n",
            "spark-3.3.2-bin-hadoop3/python/dist/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/Makefile\n",
            "spark-3.3.2-bin-hadoop3/python/docs/make.bat\n",
            "spark-3.3.2-bin-hadoop3/python/docs/make2.bat\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_static/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_static/copybutton.js\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_static/css/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_templates/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_templates/autosummary/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/conf.py\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/contributing.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/debugging.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/setting_ide.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/development/testing.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/getting_started/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/getting_started/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/getting_started/install.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/getting_started/quickstart_df.ipynb\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/getting_started/quickstart_ps.ipynb\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/koalas_to_pyspark.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/migration_guide/pyspark_3.2_to_3.3.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/extensions.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/frame.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/general_functions.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/groupby.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/indexing.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/io.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/ml.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/series.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.pandas/window.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/avro.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/catalog.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/column.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/configuration.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/core_classes.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/data_types.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/dataframe.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/functions.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/grouping.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/io.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/observation.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/row.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/spark_session.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.sql/window.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ss/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ss/core_classes.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ss/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ss/io.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.ss/query_management.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/faq.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/options.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/types.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/sql/\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/sql/arrow_pandas.rst\n",
            "spark-3.3.2-bin-hadoop3/python/docs/source/user_guide/sql/index.rst\n",
            "spark-3.3.2-bin-hadoop3/python/lib/\n",
            "spark-3.3.2-bin-hadoop3/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip\n",
            "spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip\n",
            "spark-3.3.2-bin-hadoop3/python/mypy.ini\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/__pycache__/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/_globals.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/_typing.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/accumulators.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/broadcast.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/conf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/daemon.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/files.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/find_spark_home.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/install.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/instrumentation_utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/java_gateway.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/join.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/_typing.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/classification.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/clustering.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/common.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/evaluation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/feature.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/fpm.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/image.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/linalg/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/param/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/param/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/param/shared.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/pipeline.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/recommendation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/regression.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/stat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_classification.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_clustering.yaml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_evaluation.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_feature.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_param.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_readable.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tests/typing/test_regression.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tree.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/tuning.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/ml/wrapper.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/classification.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/clustering.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/common.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/evaluation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/feature.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/fpm.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/linalg/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/random.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/random.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/recommendation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/regression.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/stat/test.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/tree.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/mllib/util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/_typing.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/accessors.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/categorical.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/config.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/binary_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/boolean_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/categorical_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/complex_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/date_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/datetime_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/null_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/num_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/string_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/timedelta_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/data_type_ops/udt_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/datetimes.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/exceptions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/extensions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/frame.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/generic.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/groupby.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/category.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/datetimes.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/multi.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/numeric.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexes/timedelta.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/indexing.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/internal.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/common.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/frame.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/general_functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/groupby.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/indexes.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/series.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/missing/window.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/ml.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/mlflow.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/namespace.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/numpy_compat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/plot/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/plot/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/plot/core.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/plot/matplotlib.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/plot/plotly.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/series.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/spark/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/spark/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/spark/accessors.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/spark/functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/spark/utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/sql_formatter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/sql_processor.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/strings.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_timedelta_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_base.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_category.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_datetime.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_timedelta.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_categorical.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_config.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_csv.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_conversion.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_default_index.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_expanding.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_extension.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_frame_spark.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_groupby.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_indexing.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_indexops_spark.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_internal.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_namespace.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_numpy_compat.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_repr.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_reshape.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_rolling.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_series.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_series_conversion.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_series_datetime.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_series_string.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_spark_functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_sql.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_stats.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_typedef.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/tests/test_window.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/typedef/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/typedef/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/typedef/typehints.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/usage_logging/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/usage_logging/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/usage_logging/usage_logger.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/pandas/window.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/profiler.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/py.typed\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/python/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/python/pyspark/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/rddsampler.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/information.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/profile.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/requests.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/resultiterable.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/serializers.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/shell.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/shuffle.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/_typing.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/avro/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/avro/functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/catalog.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/column.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/conf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/group.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/observation.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/types.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/sql_formatter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/streaming.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_arrow_map.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_typehints_with_future_annotations.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_udf_profiler.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_column.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_dataframe.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_functions.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_readwriter.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_session.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/tests/typing/test_udf.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/types.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/udf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/sql/window.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/statcounter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/status.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/storagelevel.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/dstream.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/kinesis.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/listener.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/streaming/util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/taskcontext.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/mllibutils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/mlutils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/pandasutils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/sqlutils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/streamingutils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/testing/utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/__init__.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_conf.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_context.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_daemon.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_join.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_profiler.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_rdd.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_serializers.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_statcounter.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/test_worker.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/typing/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/typing/test_context.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/typing/test_core.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/typing/test_rdd.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/tests/typing/test_resultiterable.yml\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/traceback_utils.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/util.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/version.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark/worker.py\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/requires.txt\n",
            "spark-3.3.2-bin-hadoop3/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.3.2-bin-hadoop3/python/run-tests\n",
            "spark-3.3.2-bin-hadoop3/python/run-tests-with-coverage\n",
            "spark-3.3.2-bin-hadoop3/python/run-tests.py\n",
            "spark-3.3.2-bin-hadoop3/python/setup.cfg\n",
            "spark-3.3.2-bin-hadoop3/python/setup.py\n",
            "spark-3.3.2-bin-hadoop3/python/test_coverage/\n",
            "spark-3.3.2-bin-hadoop3/python/test_coverage/conf/\n",
            "spark-3.3.2-bin-hadoop3/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.3.2-bin-hadoop3/python/test_coverage/coverage_daemon.py\n",
            "spark-3.3.2-bin-hadoop3/python/test_coverage/sitecustomize.py\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/hello/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/hello/hello.txt\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/hello/sub_hello/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/ages.csv\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/people.json\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/people1.json\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/people_array.json\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/streaming/\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/sql/text-test.txt\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/userlib-0.1.zip\n",
            "spark-3.3.2-bin-hadoop3/python/test_support/userlibrary.py\n",
            "spark-3.3.2-bin-hadoop3/sbin/\n",
            "spark-3.3.2-bin-hadoop3/sbin/decommission-slave.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/decommission-worker.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/slaves.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/spark-config.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/spark-daemon.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/spark-daemons.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-all.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-history-server.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-master.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-slave.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-slaves.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-thriftserver.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-worker.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/start-workers.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-all.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-history-server.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-master.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-slave.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-slaves.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-thriftserver.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-worker.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/stop-workers.sh\n",
            "spark-3.3.2-bin-hadoop3/sbin/workers.sh\n",
            "spark-3.3.2-bin-hadoop3/yarn/\n",
            "spark-3.3.2-bin-hadoop3/yarn/spark-3.3.2-yarn-shuffle.jar\n",
            "Collecting pyspark==3.2.1\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.3 (from pyspark==3.2.1)\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m199.0/199.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853629 sha256=10fa571d18c1e795a6c740e6d092e7e386bc9f65ade1857c85db5627a90fd6ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/97/bd/52908574a60b5f8e3dc4dc5a0b5be8a59ac20986ee51c2611b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar zxvf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "!pip install pyspark==3.2.1\n",
        "!pip install --upgrade findspark\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 1\n",
        "\n",
        "PYSPARK SESSION:\n",
        "\n",
        "To create a PySpark session, you need to import the `SparkSession` class from the `pyspark.sql` module. Here's an example of creating a PySpark session and explaining the functions used:\n"
      ],
      "metadata": {
        "id": "ocFHSvwfCwdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "Vuf7d7nWDEsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the functions used:\n",
        "\n",
        "1. *'SparkSession.builder`* : Creates a builder for the SparkSession.\n",
        "\n",
        "2. *`appName(\"MySparkApp\")`*: Sets the name of the Spark application.\n",
        "\n",
        "3. *`config(\"spark.some.config.option\", \"some-value\")`*: Sets configuration options for Spark. It allows you to specify various configuration properties based on your needs. In this example, we set a configuration option with a key-value pair.\n",
        "\n",
        "4. *`getOrCreate()`*: Retrieves an existing SparkSession or creates a new one if it doesn't exist.\n",
        "\n",
        "To add more paths, such as HDFS paths, you can use the `config()` function to set the required configurations. Here's an example:\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:port\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "In this example, we add the HDFS path by setting the \"spark.hadoop.fs.defaultFS\" configuration option to the desired HDFS URL.\n",
        "\n",
        "\n",
        "\n",
        "We are using in our scripts:\n",
        "\n",
        "**spark = SparkSession.builder.appName(Script_name).master(\"spark://master:7077\")\\\n",
        "                    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
        "                    .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
        "                    .config(\"spark.memory.offHeap.size\",\"10g\")\\\n",
        "                    .config(\"spark.driver.memory\",\"32g\")\\\n",
        "                    .config(\"spark.executor.memory\",\"32g\")\\\n",
        "                    .config(\"spark.driver.maxResultSize\",\"0\")\\\n",
        "                    .config('spark.sql.debug.maxToStringFields','1000')\\\n",
        "                    .getOrCreate()**\n",
        "\n",
        "*master(\"spark://master:7077\")*: indicating that the Spark application will connect to a Spark cluster running on the specified master node\n",
        "\n",
        "*config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")* : Configures the Spark application to use the KryoSerializer for object serialization. KryoSerializer is a high-performance serializer in Spark that provides better performance compared to the default Java serializer.\n",
        "\n",
        "*config(\"spark.memory.offHeap.enabled\",'true')* : Enables off-heap memory management in Spark. Off-heap memory is memory that is not managed by the JVM and can be beneficial for reducing the memory overhead of objects stored in Spark.\n",
        "\n",
        "*config(\"spark.memory.offHeap.size\",\"10g\")* :  Sets the size of the off-heap memory to 10 gigabytes. This determines the amount of memory allocated outside the JVM heap for caching and other Spark operations.\n",
        "\n",
        "*config(\"spark.driver.memory\",\"32g\")\n",
        " config(\"spark.executor.memory\",\"32g\")* : Sets the amount of memory allocated to the driver program and executor running on the Spark master node. In this case, it is set to 32 gigabytes.\n",
        "\n",
        "*config(\"spark.driver.maxResultSize\",\"0\")* : Sets the maximum size of the result that can be returned to the driver program. In this case, it is set to 0, indicating that there is no maximum limit on the result size\n",
        "\n",
        "*config('spark.sql.debug.maxToStringFields','1000')* : Sets the maximum number of fields to display when converting a DataFrame to a string for debugging purposes. Here, it is set to 1000, allowing a large number of fields to be displayed\n"
      ],
      "metadata": {
        "id": "_enDO4RyDMsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignments on creating different types of PySpark sessions:**\n",
        "\n",
        "1. Create a PySpark session with any app name and a specific master URL(spark://master:7077)\n",
        "\n",
        "2.  Create a PySpark session with additional Spark configuration options like executor memory - 2gb and driver memory - 1gb"
      ],
      "metadata": {
        "id": "OprUOdcmIzaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dataframes:"
      ],
      "metadata": {
        "id": "qRS0IRHoKOd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Creating DataFrames from Existing Data Structures:\n"
      ],
      "metadata": {
        "id": "aMYIvZE5JOCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Creating a DataFrame from a list\n",
        "data = [(\"Aman\", 25), (\"Abhi\", 30), (\"Annu\", 35)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RucMvJJaEG",
        "outputId": "77110a4c-e178-44f2-d208-205863cd0c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|Aman| 25|\n",
            "|Abhi| 30|\n",
            "|Annu| 35|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Reading Data from External Sources:\n",
        "\n",
        "The **header** parameter is used to indicate whether the first row of the file contains the column names or headers. When header=True, PySpark will consider the first row of the file as the header and use it to name the columns in the DataFrame.\n",
        "\n",
        "The **schema** parameter is used to infer the schema of the DataFrame based on the data in the file. When schema=True, PySpark will examine the data in the file and attempt to infer the data types of each column.\n",
        "\n",
        "By default, header is set to False, and PySpark assumes that the file does not contain a header row. Similarly, schema is set to None, and PySpark infers the schema based on the data types it detects in the file.\n"
      ],
      "metadata": {
        "id": "87C5-fd-Jf3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Reading a CSV file into a DataFrame\n",
        "house_df = spark.read.csv(\"/content/sample_data/california_housing_train.csv\", header=True, inferSchema=True)\n",
        "house_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSahW9heJnHq",
        "outputId": "dfd0ad9b-09ba-4fac-8b10-5ec0ce1b80b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -114.31|   34.19|              15.0|     5612.0|        1283.0|    1015.0|     472.0|       1.4936|           66900.0|\n",
            "|  -114.47|    34.4|              19.0|     7650.0|        1901.0|    1129.0|     463.0|         1.82|           80100.0|\n",
            "|  -114.56|   33.69|              17.0|      720.0|         174.0|     333.0|     117.0|       1.6509|           85700.0|\n",
            "|  -114.57|   33.64|              14.0|     1501.0|         337.0|     515.0|     226.0|       3.1917|           73400.0|\n",
            "|  -114.57|   33.57|              20.0|     1454.0|         326.0|     624.0|     262.0|        1.925|           65500.0|\n",
            "|  -114.58|   33.63|              29.0|     1387.0|         236.0|     671.0|     239.0|       3.3438|           74000.0|\n",
            "|  -114.58|   33.61|              25.0|     2907.0|         680.0|    1841.0|     633.0|       2.6768|           82400.0|\n",
            "|  -114.59|   34.83|              41.0|      812.0|         168.0|     375.0|     158.0|       1.7083|           48500.0|\n",
            "|  -114.59|   33.61|              34.0|     4789.0|        1175.0|    3134.0|    1056.0|       2.1782|           58400.0|\n",
            "|   -114.6|   34.83|              46.0|     1497.0|         309.0|     787.0|     271.0|       2.1908|           48100.0|\n",
            "|   -114.6|   33.62|              16.0|     3741.0|         801.0|    2434.0|     824.0|       2.6797|           86500.0|\n",
            "|   -114.6|    33.6|              21.0|     1988.0|         483.0|    1182.0|     437.0|        1.625|           62000.0|\n",
            "|  -114.61|   34.84|              48.0|     1291.0|         248.0|     580.0|     211.0|       2.1571|           48600.0|\n",
            "|  -114.61|   34.83|              31.0|     2478.0|         464.0|    1346.0|     479.0|        3.212|           70400.0|\n",
            "|  -114.63|   32.76|              15.0|     1448.0|         378.0|     949.0|     300.0|       0.8585|           45000.0|\n",
            "|  -114.65|   34.89|              17.0|     2556.0|         587.0|    1005.0|     401.0|       1.6991|           69100.0|\n",
            "|  -114.65|    33.6|              28.0|     1678.0|         322.0|     666.0|     256.0|       2.9653|           94900.0|\n",
            "|  -114.65|   32.79|              21.0|       44.0|          33.0|      64.0|      27.0|       0.8571|           25000.0|\n",
            "|  -114.66|   32.74|              17.0|     1388.0|         386.0|     775.0|     320.0|       1.2049|           44000.0|\n",
            "|  -114.67|   33.92|              17.0|       97.0|          24.0|      29.0|      15.0|       1.2656|           27500.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reading a text file:\n",
        "\n",
        "- Syntax: df = spark.read.text(\"path/to/file.txt\")\n",
        "- Usage: Reads a text file and creates a DataFrame where each line of the file is a row in the DataFrame.\n",
        "- Example Task: Read a text file and display its contents as a DataFrame."
      ],
      "metadata": {
        "id": "53CjjUWZzsPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EoCuCgkmzsLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = spark.read.text(\"/content/sample_data/practice.txt\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQL8Zk7Bzoq-",
        "outputId": "56d4e940-9730-4dab-9d34-f44ce43f7b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|Name|Age|\n",
            "+----+---+\n",
            "|Aman| 25|\n",
            "|Abhi| 30|\n",
            "|Annu| 35|\n",
            "+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Write CSV File:\n",
        "- Explanation: The write.csv() function in PySpark is used to write a DataFrame to a CSV file. It saves the contents of the DataFrame as comma-separated values (CSV) format.\n",
        "- Usage: path/to/save.csv: Specifies the path where the CSV file will be saved.\n",
        "- mode: Optional parameter to specify the write mode. It can take values like \"overwrite\" (default), \"append\", or \"ignore\".\n",
        "- header: Optional parameter to specify whether to include the header row in the CSV file. By default, it is set to True."
      ],
      "metadata": {
        "id": "e56SZ8XN0YyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Transforming an Existing DataFrame :\n",
        "\n",
        "You can create a new DataFrame by applying transformations on an existing DataFrame. PySpark provides various transformations like select, filter, groupBy, etc."
      ],
      "metadata": {
        "id": "xgQ9c7_1KfdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Creating a DataFrame from a list\n",
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "# Creating a new dataframe using a specific column of an existing dataframe\n",
        "new_df=df.select('Name')\n",
        "new_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP2X6k-2K4Yd",
        "outputId": "8513eda7-d840-4e28-b347-1c942bfd3122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|Name|\n",
            "+----+\n",
            "|Aman|\n",
            "|Abhi|\n",
            "|Annu|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment on creating a dataframe** :\n",
        " 1. Create a DataFrame from a sample california_housing_train.csv  file and filter rows where population is greater than 1000.\n",
        "\n"
      ],
      "metadata": {
        "id": "i8y2HxZjMDAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformations:"
      ],
      "metadata": {
        "id": "05Zpf7dOMnS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add Column:\n",
        "\n",
        "- Explanation: Adding a column to a DataFrame allows you to introduce new computed or derived values based on existing columns.\n",
        "- Syntax: `df.withColumn(\"new_column_name\", <expression>)`\n",
        "\n",
        "- Usage: It is used when you want to add a new column to an existing DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "AK-GlO7pMtZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df=df.withColumn(\"Total marks\", col(\"Maths\") + col(\"Hindi\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j1C4tfNM2SD",
        "outputId": "1ee13dea-ac13-499c-d85d-b36432573c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-----+-----------+\n",
            "|Name|Age|Maths|Hindi|Total marks|\n",
            "+----+---+-----+-----+-----------+\n",
            "|Aman| 25|  100|   50|        150|\n",
            "|Abhi| 30|   80|   80|        160|\n",
            "|Annu| 35|   60|   70|        130|\n",
            "+----+---+-----+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Add a new column named \"total_income\" to a DataFrame `df` by multiplying the\n",
        "\"population\" column with the \"median_income\" column.\n"
      ],
      "metadata": {
        "id": "M1A5gYVbNzSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. count:\n",
        "\n",
        "- Syntax: df.count()\n",
        "- Usage: Returns the number of rows in the DataFrame.\n",
        "- Example Task: Count the number of records in a DataFrame."
      ],
      "metadata": {
        "id": "XkhdNJOKyFB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count = df.count()\n",
        "print(\"Number of records: \", df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsJC3L7ByQ49",
        "outputId": "2b36980a-2bb2-46a9-b000-a670f8dd6183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Print Columns:\n",
        "\n",
        "- Syntax: df.columns\n",
        "- Usage: Returns a list of column names in the DataFrame.\n",
        "- Example Task: Print the names of the columns in a DataFrame."
      ],
      "metadata": {
        "id": "7vHvX9FMyqfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = df.columns\n",
        "print(\"Columns: \", columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj0oxShVy4WW",
        "outputId": "c6a7cc9d-e9cc-4668-99c0-2934dcdb71e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:  ['Name', 'Age', 'Maths', 'Hindi', 'Total marks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. print schema:\n",
        "\n",
        "- Syntax: df.printSchema()\n",
        "- Usage: Prints the schema of the DataFrame, which includes column names and their data types.\n",
        "- Example Task: Print the schema of a DataFrame.\n"
      ],
      "metadata": {
        "id": "JESzElcXy_iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQYWhLeNzOqC",
        "outputId": "26c811f7-b98e-46f0-ba5c-fc098a8cb521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- Maths: long (nullable = true)\n",
            " |-- Hindi: long (nullable = true)\n",
            " |-- Total marks: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Drop Column:"
      ],
      "metadata": {
        "id": "xRhYjO7jZ4jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Explanation: Dropping a column removes it from the DataFrame, reducing the number of columns.\n",
        "- Syntax: `df.drop(\"column_name\")`\n",
        "- Usage: It is used when you want to remove a specific column from a DataFrame.\n"
      ],
      "metadata": {
        "id": "OZSOwzejaEzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "df = df.drop(\"Age\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQnqv66VaN43",
        "outputId": "8b85806b-fdb2-411b-e0e5-7b952e988ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----+\n",
            "|Name|Maths|Hindi|\n",
            "+----+-----+-----+\n",
            "|Aman|  100|   50|\n",
            "|Abhi|   80|   80|\n",
            "|Annu|   60|   70|\n",
            "+----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Task**: Remove the column named \"median_income\" from a DataFrame `df`."
      ],
      "metadata": {
        "id": "lZJUS-_Sa3KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Select Column:\n",
        "- Explanation: Selecting columns allows you to choose specific columns from a DataFrame for further processing.\n",
        "- Syntax: `df.select(\"column1\", \"column2\", ...)`\n",
        "- Usage: It is used when you want to work with specific columns in a DataFrame.\n"
      ],
      "metadata": {
        "id": "54WzmIBAbXgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"Name\", \"Hindi\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v4BZSRVbk5A",
        "outputId": "1b3600bd-40d4-4645-f37b-c3ae564501f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|Name|Hindi|\n",
            "+----+-----+\n",
            "|Aman|   50|\n",
            "|Abhi|   80|\n",
            "|Annu|   70|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** : Select the columns total_rooms|total_bedrooms|population|households|median_income from a DataFrame `df`."
      ],
      "metadata": {
        "id": "zTkgNy8Bb3qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Filter Column:\n",
        "- Explanation: Filtering allows you to extract rows from a DataFrame that satisfy a certain condition.\n",
        "- Syntax: `df.filter(<condition>)` or `df.where(<condition>)`\n",
        "- Usage: It is used when you want to subset a DataFrame based on certain conditions.\n"
      ],
      "metadata": {
        "id": "M1N55xHTcQ8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "filtered_df = df.filter(df[\"Age\"] > 30)\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuF8yBiYcopj",
        "outputId": "4950ca17-b494-4e91-8970-8ba671e5f8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-----+\n",
            "|Name|Age|Maths|Hindi|\n",
            "+----+---+-----+-----+\n",
            "|Annu| 35|   60|   70|\n",
            "+----+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Task**: Filter the DataFrame `df` to include only the rows where the \"total_rooms\" column is greater than 2550."
      ],
      "metadata": {
        "id": "75RReloldC-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Collect:\n",
        "- Explanation: Collecting a DataFrame returns all the rows as a list of Row objects, which can be useful for further processing. Retrieve all the elements of the dataset (from all nodes) to the driver node\n",
        "- Syntax: `df.collect()`\n",
        "- Usage: It is used when you want to retrieve all the rows from a DataFrame and perform operations on the collected data.\n",
        "\n"
      ],
      "metadata": {
        "id": "3QJv-4vsdZka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collected_data = df.collect()\n",
        "for row in collected_data:\n",
        "    print(row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VYblgBsdpMh",
        "outputId": "c902202d-f769-4740-e6d8-3ac11273171c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Name='Aman', Age=25, Maths=100, Hindi=50)\n",
            "Row(Name='Abhi', Age=30, Maths=80, Hindi=80)\n",
            "Row(Name='Annu', Age=35, Maths=60, Hindi=70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Task: Collect all the rows from a DataFrame `df` and print them."
      ],
      "metadata": {
        "id": "1iaddx1Gd7lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. First:\n",
        "- Explanation: The `first()` function returns the first row of a DataFrame.\n",
        "- Syntax: `df.first()`\n",
        "- Usage: It is used when you want to retrieve the first row of a DataFrame.\n",
        "- Task: Get the first row from a DataFrame `df` and print it.\n"
      ],
      "metadata": {
        "id": "PhB3Uue8e7lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_row = df.first()\n",
        "print(first_row)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEL1IBkufJqm",
        "outputId": "5d7b908c-46f6-4d93-df2e-b404ab2b935b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(Name='Aman', Age=25, Maths=100, Hindi=50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Get the first row from a DataFrame `df` and print it."
      ],
      "metadata": {
        "id": "NjFWwlvqfY5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAY 2**"
      ],
      "metadata": {
        "id": "NNpMqkMcxVp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **orderBy/sort**:\n",
        "- Explanation: `orderBy` and `sort` are used to sort the rows in a DataFrame based on one or more columns in ascending or descending order.\n",
        "- Syntax:\n",
        "```python\n",
        "df.orderBy(\"column1\")  # Sorts in ascending order\n",
        "df.orderBy(df.column1.desc())  # Sorts in descending order\n",
        "\n",
        "Sorting the DataFrame by the \"age\" column in descending order."
      ],
      "metadata": {
        "id": "5M_lEpLXRpU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "sorted_df = df.orderBy(df.Age.desc())\n",
        "sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlxwR6V_R6J9",
        "outputId": "20d7585d-60dc-439e-fb80-abb53f74b61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-----+\n",
            "|Name|Age|Maths|Hindi|\n",
            "+----+---+-----+-----+\n",
            "|Annu| 35|   60|   70|\n",
            "|Abhi| 30|   80|   80|\n",
            "|Aman| 25|  100|   50|\n",
            "+----+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Task - Sort the df on the basis of population in ascending order."
      ],
      "metadata": {
        "id": "hJ1t4MhKT9ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using sort :  Sort DataFrame in ascending order based on a single column:"
      ],
      "metadata": {
        "id": "cOC3md27UfAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "# Sort the DataFrame in ascending order based on the \"Age\" column\n",
        "sorted_df = df.sort(col(\"Age\").desc())\n",
        "sorted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw6NVM1AjJPR",
        "outputId": "6b115918-1d63-499e-a59a-7763219f64d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-----+\n",
            "|Name|Age|Maths|Hindi|\n",
            "+----+---+-----+-----+\n",
            "|Annu| 35|   60|   70|\n",
            "|Abhi| 30|   80|   80|\n",
            "|Aman| 25|  100|   50|\n",
            "+----+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Sort DataFrame in descending order based on multiple columns:\n",
        "\n"
      ],
      "metadata": {
        "id": "3P_TT-LVje3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "data = [(\"Aman\", 25,100,50), (\"Abhi\", 30,80,80), (\"Annu\", 35,60,70)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\",\"Maths\",\"Hindi\"])\n",
        "\n",
        "# Sort the DataFrame in descending order based on the \"Age\" and \"Name\" columns\n",
        "sorted_df = df.sort(col(\"Age\").desc(), col(\"Name\").desc())\n",
        "sorted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5RW50fFjg1j",
        "outputId": "9917617d-6064-4022-b117-f12e31836d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-----+\n",
            "|Name|Age|Maths|Hindi|\n",
            "+----+---+-----+-----+\n",
            "|Annu| 35|   60|   70|\n",
            "|Abhi| 30|   80|   80|\n",
            "|Aman| 25|  100|   50|\n",
            "+----+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "data = [(\"Aman\",'IT',30000,'New Delhi'), (\"Abhi\", 'UI',35000,'Haryana'), (\"Annu\", 'IT',35000,'Punjab')]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\"])\n",
        "print('Original dataframe')\n",
        "df.show()\n",
        "\n",
        "grouped_df = df.groupBy(\"department\").agg({\"salary\": \"sum\"})\n",
        "print('After group by on department and sum(salary)')\n",
        "grouped_df.show()\n",
        "\n",
        "grouped_df = df.groupBy(\"department\").agg({\"salary\": \"max\"})\n",
        "print('After group by on department and max(salary)')\n",
        "grouped_df.show()\n",
        "\n",
        "# grouped_df = df.groupBy(\"department\").agg({\"salary\": \"min\"})\n",
        "grouped_df = df.groupBy(\"department\").min(\"salary\")\n",
        "\n",
        "print('After group by on department and min(salary)')\n",
        "grouped_df.show()\n",
        "\n",
        "# grouped_df = df.groupBy(\"department\").agg({\"salary\": \"avg\"})\n",
        "grouped_df = df.groupBy(\"department\").avg(\"salary\")\n",
        "print('After group by on department and avg(salary)')\n",
        "grouped_df.show()\n",
        "\n",
        "grouped_df = df.groupBy(\"department\").mean(\"salary\")\n",
        "print('After group by on department and max(mean)')\n",
        "grouped_df.show()\n",
        "\n",
        "grouped_df = df.groupBy(\"department\").count()\n",
        "print('After group by on department count')\n",
        "grouped_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8MMWXuxkeJJ",
        "outputId": "f159703f-f15f-4861-d19e-089d4db56b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe\n",
            "+----+----------+------+---------+\n",
            "|Name|department|salary|    State|\n",
            "+----+----------+------+---------+\n",
            "|Aman|        IT| 30000|New Delhi|\n",
            "|Abhi|        UI| 35000|  Haryana|\n",
            "|Annu|        IT| 35000|   Punjab|\n",
            "+----+----------+------+---------+\n",
            "\n",
            "After group by on department and sum(salary)\n",
            "+----------+-----------+\n",
            "|department|sum(salary)|\n",
            "+----------+-----------+\n",
            "|        IT|      65000|\n",
            "|        UI|      35000|\n",
            "+----------+-----------+\n",
            "\n",
            "After group by on department and max(salary)\n",
            "+----------+-----------+\n",
            "|department|max(salary)|\n",
            "+----------+-----------+\n",
            "|        IT|      35000|\n",
            "|        UI|      35000|\n",
            "+----------+-----------+\n",
            "\n",
            "After group by on department and min(salary)\n",
            "+----------+-----------+\n",
            "|department|min(salary)|\n",
            "+----------+-----------+\n",
            "|        IT|      30000|\n",
            "|        UI|      35000|\n",
            "+----------+-----------+\n",
            "\n",
            "After group by on department and avg(salary)\n",
            "+----------+-----------+\n",
            "|department|avg(salary)|\n",
            "+----------+-----------+\n",
            "|        IT|    32500.0|\n",
            "|        UI|    35000.0|\n",
            "+----------+-----------+\n",
            "\n",
            "After group by on department and max(mean)\n",
            "+----------+-----------+\n",
            "|department|avg(salary)|\n",
            "+----------+-----------+\n",
            "|        IT|    32500.0|\n",
            "|        UI|    35000.0|\n",
            "+----------+-----------+\n",
            "\n",
            "After group by on department count\n",
            "+----------+-----+\n",
            "|department|count|\n",
            "+----------+-----+\n",
            "|        IT|    2|\n",
            "|        UI|    1|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **groupBy:**\n",
        "- Explanation: `groupBy` is used to group the rows in a DataFrame based on one or more columns.\n",
        "- Syntax:\n",
        "```python\n",
        "df.groupBy(\"column1\")\n",
        "```\n",
        "- Usage:\n",
        "```python\n",
        "df.groupBy(\"column1\").agg({\"column2\": \"sum\"})\n",
        "```\n",
        "- Task: Group the DataFrame by the \"department\" column and calculate the total salary for each department.\n",
        "```python\n",
        "grouped_df = df.groupBy(\"department\").agg({\"salary\": \"sum\"})\n",
        "```"
      ],
      "metadata": {
        "id": "XHaNxjRCj6E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Using Multiple Columns**"
      ],
      "metadata": {
        "id": "A9jTunlBnr2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "data = [(\"Aman\",'IT',30000,'New Delhi',2000), (\"Abhi\", 'UI',35000,'Haryana',1200),\\\n",
        " (\"Annu\", 'IT',35000,'Punjab',1200),(\"Akshay\", 'IT',25000,'New Delhi',1500),(\"Abhi\", 'UI',40000,'Haryana',1000)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\",\"Incentive\"])\n",
        "print('Original dataframe')\n",
        "df.show()\n",
        "\n",
        "grouped_df = df.groupBy(\"department\",\"state\").sum(\"salary\",\"Incentive\")\n",
        "print('After group by on department and state')\n",
        "grouped_df.show()\n",
        "\n",
        "print('Adding name column in group by')\n",
        "df.groupBy(\"Name\",\"department\",\"state\").count().show()\n",
        "df.groupBy(\"Name\",\"department\",\"state\").sum(\"salary\",\"Incentive\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhJgBiyQnz0I",
        "outputId": "b076d6e6-18a8-452e-d864-59422270a061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe\n",
            "+------+----------+------+---------+---------+\n",
            "|  Name|department|salary|    State|Incentive|\n",
            "+------+----------+------+---------+---------+\n",
            "|  Aman|        IT| 30000|New Delhi|     2000|\n",
            "|  Abhi|        UI| 35000|  Haryana|     1200|\n",
            "|  Annu|        IT| 35000|   Punjab|     1200|\n",
            "|Akshay|        IT| 25000|New Delhi|     1500|\n",
            "|  Abhi|        UI| 40000|  Haryana|     1000|\n",
            "+------+----------+------+---------+---------+\n",
            "\n",
            "After group by on department and state\n",
            "+----------+---------+-----------+--------------+\n",
            "|department|    state|sum(salary)|sum(Incentive)|\n",
            "+----------+---------+-----------+--------------+\n",
            "|        IT|New Delhi|      55000|          3500|\n",
            "|        UI|  Haryana|      75000|          2200|\n",
            "|        IT|   Punjab|      35000|          1200|\n",
            "+----------+---------+-----------+--------------+\n",
            "\n",
            "Adding name column in group by\n",
            "+------+----------+---------+-----+\n",
            "|  Name|department|    state|count|\n",
            "+------+----------+---------+-----+\n",
            "|  Abhi|        UI|  Haryana|    2|\n",
            "|  Aman|        IT|New Delhi|    1|\n",
            "|Akshay|        IT|New Delhi|    1|\n",
            "|  Annu|        IT|   Punjab|    1|\n",
            "+------+----------+---------+-----+\n",
            "\n",
            "+------+----------+---------+-----------+--------------+\n",
            "|  Name|department|    state|sum(salary)|sum(Incentive)|\n",
            "+------+----------+---------+-----------+--------------+\n",
            "|  Abhi|        UI|  Haryana|      75000|          2200|\n",
            "|  Aman|        IT|New Delhi|      30000|          2000|\n",
            "|Akshay|        IT|New Delhi|      25000|          1500|\n",
            "|  Annu|        IT|   Punjab|      35000|          1200|\n",
            "+------+----------+---------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Using more aggregates:**"
      ],
      "metadata": {
        "id": "N1JyPMbgrLoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Aman\",'IT',30000,'New Delhi',2000), (\"Abhi\", 'UI',35000,'Haryana',1200),\\\n",
        " (\"Annu\", 'IT',35000,'Punjab',1200),(\"Akshay\", 'IT',25000,'New Delhi',1500),(\"Abhi\", 'UI',40000,'Haryana',1000)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\",\"Incentive\"])\n",
        "print('Original dataframe')\n",
        "df.show()\n",
        "\n",
        "df.groupBy(\"department\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"Incentive\").alias(\"sum_bonus\"), \\\n",
        "         max(\"Incentive\").alias(\"max_bonus\") \\\n",
        "     ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu_RkXXNrQpc",
        "outputId": "40cebfd3-b867-4ae7-bfda-1f631701bc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe\n",
            "+------+----------+------+---------+---------+\n",
            "|  Name|department|salary|    State|Incentive|\n",
            "+------+----------+------+---------+---------+\n",
            "|  Aman|        IT| 30000|New Delhi|     2000|\n",
            "|  Abhi|        UI| 35000|  Haryana|     1200|\n",
            "|  Annu|        IT| 35000|   Punjab|     1200|\n",
            "|Akshay|        IT| 25000|New Delhi|     1500|\n",
            "|  Abhi|        UI| 40000|  Haryana|     1000|\n",
            "+------+----------+------+---------+---------+\n",
            "\n",
            "+----------+----------+----------+---------+---------+\n",
            "|department|sum_salary|avg_salary|sum_bonus|max_bonus|\n",
            "+----------+----------+----------+---------+---------+\n",
            "|        UI|     75000|   37500.0|     2200|     1200|\n",
            "|        IT|     90000|   30000.0|     4700|     2000|\n",
            "+----------+----------+----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Using filter on aggregate data**"
      ],
      "metadata": {
        "id": "i2PrJOaHsFnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Aman\",'IT',30000,'New Delhi',2000), (\"Abhi\", 'UI',35000,'Haryana',1200),\\\n",
        " (\"Annu\", 'IT',35000,'Punjab',1200),(\"Akshay\", 'IT',25000,'New Delhi',1500),(\"Abhi\", 'UI',40000,'Haryana',1000)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\",\"Incentive\"])\n",
        "print('Original dataframe')\n",
        "df.show()\n",
        "\n",
        "print('Without filter')\n",
        "df.groupBy(\"department\",\"State\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"Incentive\").alias(\"sum_bonus\"), \\\n",
        "         max(\"Incentive\").alias(\"max_bonus\")).show()\n",
        "\n",
        "print('With filter condition where bonus is greater than 2500')\n",
        "df.groupBy(\"department\",\"State\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"Incentive\").alias(\"sum_bonus\"), \\\n",
        "         max(\"Incentive\").alias(\"max_bonus\")) \\\n",
        "        .where(col(\"sum_bonus\") >= 2500).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGSY7dw_sKsm",
        "outputId": "e3c45704-9523-46db-dc3c-0e15cae13fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe\n",
            "+------+----------+------+---------+---------+\n",
            "|  Name|department|salary|    State|Incentive|\n",
            "+------+----------+------+---------+---------+\n",
            "|  Aman|        IT| 30000|New Delhi|     2000|\n",
            "|  Abhi|        UI| 35000|  Haryana|     1200|\n",
            "|  Annu|        IT| 35000|   Punjab|     1200|\n",
            "|Akshay|        IT| 25000|New Delhi|     1500|\n",
            "|  Abhi|        UI| 40000|  Haryana|     1000|\n",
            "+------+----------+------+---------+---------+\n",
            "\n",
            "Without filter\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "|department|    State|sum_salary|avg_salary|sum_bonus|max_bonus|\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "|        IT|New Delhi|     55000|   27500.0|     3500|     2000|\n",
            "|        UI|  Haryana|     75000|   37500.0|     2200|     1200|\n",
            "|        IT|   Punjab|     35000|   35000.0|     1200|     1200|\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "\n",
            "With filter condition where bonus is greater than 2500\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "|department|    State|sum_salary|avg_salary|sum_bonus|max_bonus|\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "|        IT|New Delhi|     55000|   27500.0|     3500|     2000|\n",
            "+----------+---------+----------+----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task to do:**\n",
        "\n",
        "data - [(\"Aman\",'HR',20000,'New Delhi',2500), (\"Abhi\", 'IT',30000,'Haryana',1000),(\"Annu\", 'HR',35000,'New Delhi',1500),(\"Akshay\", 'UI',25000,'Gujrat',2500),(\"Abhi\", 'IT',40000,'Haryana',1000)]\n",
        "\n",
        " df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\",\"Incentive\"])\n",
        "\n",
        "- **Perform group by on \"department\" and \"state\" , sum of \"salary\" as sum_salary, max of \"Incentive\" as max_bonus, filter where \"max_bonus\" > '2000'.**"
      ],
      "metadata": {
        "id": "2f1Z89_mt8zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg,max\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Aman\",'HR',20000,'New Delhi',2500), (\"Abhi\", 'IT',30000,'Haryana',1000),(\"Annu\", 'HR',35000,'New Delhi',1500),(\"Akshay\", 'UI',25000,'Gujrat',2500),(\"Abhi\", 'IT',40000,'Haryana',1000)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"department\",\"salary\",\"State\",\"Incentive\"])\n",
        "print('Original dataframe')\n",
        "df.show()\n",
        "\n",
        "print('With filter condition where maximum bonus is greater than 1000')\n",
        "df.groupBy(\"department\",\"State\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"),\\\n",
        "         max(\"Incentive\").alias(\"max_bonus\"))\\\n",
        "         .where(col(\"max_bonus\") > 1000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6ClGUmwHj9",
        "outputId": "82debbb9-bbc9-4366-a4b0-44e341b56d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataframe\n",
            "+------+----------+------+---------+---------+\n",
            "|  Name|department|salary|    State|Incentive|\n",
            "+------+----------+------+---------+---------+\n",
            "|  Aman|        HR| 20000|New Delhi|     2500|\n",
            "|  Abhi|        IT| 30000|  Haryana|     1000|\n",
            "|  Annu|        HR| 35000|New Delhi|     1500|\n",
            "|Akshay|        UI| 25000|   Gujrat|     2500|\n",
            "|  Abhi|        IT| 40000|  Haryana|     1000|\n",
            "+------+----------+------+---------+---------+\n",
            "\n",
            "With filter condition where maximum bonus is greater than 1000\n",
            "+----------+---------+----------+---------+\n",
            "|department|    State|sum_salary|max_bonus|\n",
            "+----------+---------+----------+---------+\n",
            "|        HR|New Delhi|     55000|     2500|\n",
            "|        UI|   Gujrat|     25000|     2500|\n",
            "+----------+---------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Union and Union All:**\n",
        "- Explanation: The `union` operation combines two DataFrames vertically, appending the rows of one DataFrame to the other.If schemas are not the same it returns an error.\n",
        "The `unionAll` operation performs the same operation but includes duplicate rows.unionAll() is deprecated since Spark 2.0.0 version and replaced with union().\n",
        "\n",
        "- Syntax:\n",
        "  ```python\n",
        "  df1.union(df2)  # Union operation\n",
        "  df1.unionAll(df2)  # Union All operation\n",
        "  ```\n",
        "- Usage: Used to combine multiple DataFrames with the same schema.\n",
        "- Tasks:\n",
        "  - Task 1: Union two DataFrames and display the combined result."
      ],
      "metadata": {
        "id": "r1tGtCGktFzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data1 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = Data1, schema = columns)\n",
        "# df.printSchema()\n",
        "print('First df')\n",
        "df.show()\n",
        "\n",
        "Data2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data = Data2, schema = columns2)\n",
        "# df2.printSchema()\n",
        "print('Second df')\n",
        "df2.show()\n",
        "\n",
        "\n",
        "unionDF = df.union(df2)\n",
        "print('First df union second df')\n",
        "unionDF.show()\n",
        "\n",
        "'''Since the union() method returns all rows without distinct records,\n",
        "we will use the distinct() function to return just one record when\n",
        "duplicate exists.'''\n",
        "\n",
        "print('Using distinct')\n",
        "disDF = df.union(df2).distinct()\n",
        "disDF.show()\n",
        "\n",
        "print('Using Unionall')\n",
        "unionAllDF = df.unionAll(df2)\n",
        "unionAllDF.show()\n",
        "\n",
        "dfs = [df, df2]\n",
        "from functools import reduce\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# create merged dataframe\n",
        "df_complete = reduce(DataFrame.unionAll, dfs)\n",
        "print('Using list of dfs')\n",
        "df_complete.show()\n",
        "\n",
        "\n",
        "Data3 = [(\"Peter\",90000,\"Sales\",\"NY\",34), \\\n",
        "    (\"Carl\",9000,\"Finance\",\"CA\",24), \\\n",
        "    (\"Jason\",79000,\"Finance\",\"NY\",53), \\\n",
        "    (\"Alex\",80000,\"Marketing\",\"CA\",25), \\\n",
        "    (\"Leo\",91000,\"Marketing\",\"NY\",50)]\n",
        "columns3= [\"employee_name\",\"salary\",\"department\",\"state\",\"age\"]\n",
        "\n",
        "df3 = spark.createDataFrame(data = Data3, schema = columns3)\n",
        "# df3.printSchema()\n",
        "print('Third df')\n",
        "df3.show()\n",
        "\n",
        "\n",
        "'''unionByName works when both DataFrames have the same columns,\n",
        "but in a different order. An optional parameter was also added in\n",
        "Spark 3.1 to allow unioning slightly different schemas.'''\n",
        "\n",
        "byNameDF = df.unionByName(df3,allowMissingColumns=True)\n",
        "print('UnionByName result')\n",
        "byNameDF.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8niFycgTPs6",
        "outputId": "632c96e3-64ac-46db-8786-4cac1591ef1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First df\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "Second df\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "First df union second df\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "Using distinct\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "Using Unionall\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "Using list of dfs\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "Third df\n",
            "+-------------+------+----------+-----+---+\n",
            "|employee_name|salary|department|state|age|\n",
            "+-------------+------+----------+-----+---+\n",
            "|        Peter| 90000|     Sales|   NY| 34|\n",
            "|         Carl|  9000|   Finance|   CA| 24|\n",
            "|        Jason| 79000|   Finance|   NY| 53|\n",
            "|         Alex| 80000| Marketing|   CA| 25|\n",
            "|          Leo| 91000| Marketing|   NY| 50|\n",
            "+-------------+------+----------+-----+---+\n",
            "\n",
            "UnionByName result\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        Peter|     Sales|   NY| 90000| 34| null|\n",
            "|         Carl|   Finance|   CA|  9000| 24| null|\n",
            "|        Jason|   Finance|   NY| 79000| 53| null|\n",
            "|         Alex| Marketing|   CA| 80000| 25| null|\n",
            "|          Leo| Marketing|   NY| 91000| 50| null|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Task:** Copy the existing df you are using, modify it (change name, rearrange column) then use unionByName to merge both the dataframes."
      ],
      "metadata": {
        "id": "U5AOAeCpgFhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Renaming Columns:**\n",
        "- Explanation: Renaming columns allows you to change the names of specific columns in a DataFrame.\n",
        "- Syntax:\n",
        "  ```python\n",
        "  df.withColumnRenamed(\"old_column\", \"new_column\")\n",
        "  ```\n",
        "- Usage: Change the name of a column in a DataFrame.\n",
        "- Tasks:\n",
        "  - Task 1: Rename the \"old_name\" column to \"new_name\".\n",
        "  ```python\n",
        "  df = df.withColumnRenamed(\"old_name\", \"new_name\")\n",
        "  ```"
      ],
      "metadata": {
        "id": "BKg_wppogl2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data1 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = Data1, schema = columns)\n",
        "\n",
        "df = df.withColumnRenamed(\"bonus\", \"Incentive\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaN71sO3pP0C",
        "outputId": "824a31a5-ff79-40f1-ef93-fe397212d489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+---------+\n",
            "|employee_name|department|state|salary|age|Incentive|\n",
            "+-------------+----------+-----+------+---+---------+\n",
            "|        James|     Sales|   NY| 90000| 34|    10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|    20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|    23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|    23000|\n",
            "+-------------+----------+-----+------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**: Rename column employee_name to worker_name."
      ],
      "metadata": {
        "id": "8okqUKR3qUKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **dropDuplicates and distinct:**\n",
        "\n",
        "Explanation:Return a new DataFrame with duplicate rows removed.\n",
        "\n",
        "- Syntax:\n",
        "  ```python\n",
        "  df.dropDuplicates([\"column1\", \"column2\"])  # Specify the columns to check for duplicates\n",
        "  ```\n",
        "- Usage: Remove duplicate columns from a DataFrame.\n",
        "- Tasks:\n",
        "  - Task 1: Remove duplicate columns from the DataFrame."
      ],
      "metadata": {
        "id": "8wzrjfbggw-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000)  ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = Data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "print('dropping duplicate salary')\n",
        "df.dropDuplicates([\"salary\"]).show()\n",
        "\n",
        "print('dropping duplicate department+salary')\n",
        "df = df.dropDuplicates([\"department\",\"salary\"])\n",
        "df.show()\n",
        "\n",
        "df=df.distinct()\n",
        "print('After applying distinct')\n",
        "df.show()\n",
        "\n",
        "print('dropped duplicate columns')\n",
        "cols = (\"state\",\"state\",\"bonus\")\n",
        "df.drop(*cols).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riUDn1ePsA_v",
        "outputId": "b3f948fe-0e17-46c8-da53-da6e908b00bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "dropping duplicate salary\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "dropping duplicate department+salary\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "After applying distinct\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "dropped duplicate columns\n",
            "+-------------+----------+------+---+\n",
            "|employee_name|department|salary|age|\n",
            "+-------------+----------+------+---+\n",
            "|        Maria|   Finance| 90000| 24|\n",
            "|       Robert|     Sales| 81000| 30|\n",
            "|      Michael|     Sales| 86000| 56|\n",
            "|        James|     Sales| 90000| 34|\n",
            "+-------------+----------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** : Drop Duplicate rows from the given dataset ."
      ],
      "metadata": {
        "id": "l4yK1BwNwrp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "6. Fillna and Fill():\n",
        "- Explanation: `fillna` and `fill` operations are used to fill null or missing values in a DataFrame with specified values or predefined methods.\n",
        "- Syntax:\n",
        "  ```python\n",
        "  df.fillna(value, subset=[\"column1\", \"column2\"])  # Fill specified columns with a value\n",
        "  df.fillna({\"column1\": value1, \"column2\": value2})  # Fill specified columns with respective values\n",
        "  df.fill(value)  # Fill all columns with a value\n",
        "  ```\n",
        "- Usage: Fill null or missing values in a DataFrame with specified values or predefined methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "T_-FGMFBg1Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import Row\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "from pyspark.shell import spark\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(Roll_no=None, Date='20151231', Data='Hello'),\n",
        "    Row(Roll_no=2, Date='20160101', Data=None),\n",
        "    Row(Roll_no=3, Date=None, Data='World')\n",
        "])\n",
        "\n",
        "df.write.csv('/content/sample_data/practice1.csv', header=True, emptyValue='')\n",
        "\n",
        "df_1=spark.read.csv('/content/sample_data/practice1.csv', header=True,inferSchema=True)\n",
        "df_1.show()\n",
        "\n",
        "#Fill null values in \"column1\" with a specific value.\n",
        "df_1 = df_1.fillna({\"Roll_no\":5})\n",
        "\n",
        "#Replace 0 for null for all integer columns\n",
        "df_1 = df_1.na.fill(value=0)\n",
        "\n",
        "#Replace null on column\n",
        "df_1=df_1.na.fill(value='NoData',subset=[\"Data\"])\n",
        "df_1.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmxtvqH-8Eml",
        "outputId": "6835528d-987a-4c89-f9b6-0b0b70e84f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.10.12 (main, Jun  7 2023 12:45:35)\n",
            "Spark context Web UI available at http://d4d1483c5ba1:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1687319110394).\n",
            "SparkSession available as 'spark'.\n",
            "+-------+--------+-----+\n",
            "|Roll_no|    Date| Data|\n",
            "+-------+--------+-----+\n",
            "|      2|20160101| null|\n",
            "|      3|    null|World|\n",
            "|   null|20151231|Hello|\n",
            "+-------+--------+-----+\n",
            "\n",
            "+-------+--------+------+\n",
            "|Roll_no|    Date|  Data|\n",
            "+-------+--------+------+\n",
            "|      2|20160101|NoData|\n",
            "|      3|       0| World|\n",
            "|      5|20151231| Hello|\n",
            "+-------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Drop Null Values:\n",
        "- Explanation: The `dropna` operation removes rows containing null or missing values from a DataFrame.\n",
        "- Syntax:\n",
        "  ```python\n",
        "  df.dropna()\n",
        "  ```\n",
        "- Usage: Remove rows with null values from a DataFrame.\n"
      ],
      "metadata": {
        "id": "9i2fz_kTg9D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import Row\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# from pyspark.shell import spark\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(Roll_no=None, Date='20151231', Data='Hello'),\n",
        "    Row(Roll_no=2, Date='20160101', Data=None),\n",
        "    Row(Roll_no=None, Date=None, Data=None)\n",
        "])\n",
        "\n",
        "df.write.csv('/content/sample_data/practice12.csv', header=True, emptyValue='')\n",
        "\n",
        "df_1=spark.read.csv('/content/sample_data/practice12.csv', header=True,inferSchema=True)\n",
        "df_1.show()\n",
        "\n",
        "df_1=df_1.dropna(how=\"any\")\n",
        "df_1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UumWpa0RC84C",
        "outputId": "8cb655eb-ed69-43cc-9188-ff8e5bc844ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+-----+\n",
            "|Roll_no|    Date| Data|\n",
            "+-------+--------+-----+\n",
            "|   null|20151231|Hello|\n",
            "|      2|20160101| null|\n",
            "|   null|    null| null|\n",
            "+-------+--------+-----+\n",
            "\n",
            "+-------+----+----+\n",
            "|Roll_no|Date|Data|\n",
            "+-------+----+----+\n",
            "+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Pivoting:\n",
        "- Explanation: Pivoting transforms the data by rotating rows into columns, creating a summary table.\n",
        "- Syntax:\n",
        "\n",
        "  ```python\n",
        "  df.groupBy(\"column1\").pivot(\"column2\").agg(function)\n",
        "  ```\n",
        "\n",
        "- Usage: Perform pivot operations to create summary tables.\n",
        "\n",
        "  - Task : Pivot the DataFrame by \"column1\" with \"column2\" as columns and calculate the sum of \"column3\" for each combination.\n",
        "\n",
        "\n",
        "  ```python\n",
        "  pivoted_df = df.groupBy(\"column1\").pivot(\"column2\").sum(\"column3\")```\n"
      ],
      "metadata": {
        "id": "ncRN6S--hDDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000)  ]\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = Data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "pivoted_df = df.groupBy(\"employee_name\").pivot(\"department\").sum(\"bonus\")\n",
        "pivoted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7KtiVX7EHSV",
        "outputId": "428a47ca-e9b1-4548-f487-81de79788427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n",
            "+-------------+-------+-----+\n",
            "|employee_name|Finance|Sales|\n",
            "+-------------+-------+-----+\n",
            "|        James|   null|10000|\n",
            "|      Michael|   null|40000|\n",
            "|        Maria|  46000| null|\n",
            "|       Robert|   null|23000|\n",
            "+-------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # DAY 3"
      ],
      "metadata": {
        "id": "j_6Pt0kE0qGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Joins**:\n",
        "   - Explanation: Joins are used to combine two or more DataFrames based on a common column or key.\n",
        "   - Syntax: `df1.join(df2, on=<join_column>, how=<join_type>)`\n",
        "   - Usage: Used when you need to combine data from multiple DataFrames based on a common column.\n",
        "\n",
        "   `on` specifies the join condition (e.g., the column(s) to join on).\n",
        "\n",
        "   `how` specifies the type of join (e.g., 'inner', 'outer', 'left', 'right').\n",
        "\n",
        "   Code Example:\n",
        "   ```python\n",
        "   # Inner Join\n",
        "   df_inner = df1.join(df2, on='common_column', how='inner')\n",
        "\n",
        "   # Left Join\n",
        "   df_left = df1.join(df2, on='common_column', how='left')\n",
        "\n",
        "   # Right Join\n",
        "   df_right = df1.join(df2, on='common_column', how='right')\n",
        "\n",
        "   # Full Outer Join\n",
        "   df_outer = df1.join(df2, on='common_column', how='outer')\n",
        "   ```\n",
        "\n"
      ],
      "metadata": {
        "id": "98vhPgGt1aVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Usage :\n",
        "   - Combine two DataFrames, keeping only the matching rows.\n",
        "\n",
        "     Use an inner join (`how='inner'`).\n",
        "   - Combine two DataFrames, keeping all the rows from the left DataFrame.\n",
        "\n",
        "     Use a left join (`how='left'`).\n",
        "   - Combine two DataFrames, keeping all the rows from the right DataFrame.\n",
        "\n",
        "     Use a right join (`how='right'`).\n",
        "   - Combine two DataFrames, keeping all the rows from both DataFrames.\n",
        "\n",
        "     Use a full outer join (`how='outer'`).\n",
        "\n",
        "     ![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "O4ZSzvy017Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation**"
      ],
      "metadata": {
        "id": "qx0jj53P3GZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating two DataFrames\n",
        "df1 = spark.createDataFrame([(1, 'Amay'), (2, 'Bobby'), (3, 'Charu')], ['id', 'name'])\n",
        "print('df1')\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame([(1, 'New Delhi'), (2, 'Punjab'), (4, 'Haryana')], ['id', 'city'])\n",
        "print('df2')\n",
        "df2.show()\n",
        "\n",
        "# Inner join example\n",
        "inner_join = df1.join(df2, on='id', how='inner')\n",
        "print('INNER JOIN')\n",
        "inner_join.show()\n",
        "\n",
        "# Right join example\n",
        "right_join = df1.join(df2, on='id', how='right')\n",
        "print('RIGHT JOIN')\n",
        "right_join.show()\n",
        "\n",
        "# Left join example\n",
        "left_join = df1.join(df2, on='id', how='left')\n",
        "print('LEFT JOIN')\n",
        "left_join.show()\n",
        "\n",
        "# Full outer join example\n",
        "full_outer_join = df1.join(df2, on='id', how='outer')\n",
        "print('FULL OUTER JOIN')\n",
        "full_outer_join.show()\n",
        "\n",
        "'''leftsemi join is similar to inner join difference being leftsemi join\n",
        "returns all columns from the left dataset and ignores all columns from\n",
        "the right dataset.'''\n",
        "\n",
        "# Left semi join example\n",
        "left_semi_join = df1.join(df2, on='id', how='leftsemi')\n",
        "print('LEFT SEMI JOIN')\n",
        "left_semi_join.show()\n",
        "\n",
        "'''leftanti join does the exact opposite of the leftsemi, leftanti join returns\n",
        "only columns from the left dataset for non-matched records.'''\n",
        "\n",
        "# Left anti join example\n",
        "left_anti_join = df1.join(df2, on='id', how='leftanti')\n",
        "print('LEFT ANTI JOIN')\n",
        "left_anti_join.show()\n",
        "\n",
        "'''This join simply combines each row of the first table with each row\n",
        " of the second table'''\n",
        "\n",
        "# cross join example\n",
        "cross_join = df1.crossJoin(df2)\n",
        "print('CROSS JOIN')\n",
        "cross_join.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz9o_ObV3J7c",
        "outputId": "05459630-6b4d-4e83-f342-6a98954c401c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1| Amay|\n",
            "|  2|Bobby|\n",
            "|  3|Charu|\n",
            "+---+-----+\n",
            "\n",
            "df2\n",
            "+---+---------+\n",
            "| id|     city|\n",
            "+---+---------+\n",
            "|  1|New Delhi|\n",
            "|  2|   Punjab|\n",
            "|  4|  Haryana|\n",
            "+---+---------+\n",
            "\n",
            "INNER JOIN\n",
            "+---+-----+---------+\n",
            "| id| name|     city|\n",
            "+---+-----+---------+\n",
            "|  1| Amay|New Delhi|\n",
            "|  2|Bobby|   Punjab|\n",
            "+---+-----+---------+\n",
            "\n",
            "RIGHT JOIN\n",
            "+---+-----+---------+\n",
            "| id| name|     city|\n",
            "+---+-----+---------+\n",
            "|  1| Amay|New Delhi|\n",
            "|  2|Bobby|   Punjab|\n",
            "|  4| null|  Haryana|\n",
            "+---+-----+---------+\n",
            "\n",
            "LEFT JOIN\n",
            "+---+-----+---------+\n",
            "| id| name|     city|\n",
            "+---+-----+---------+\n",
            "|  1| Amay|New Delhi|\n",
            "|  3|Charu|     null|\n",
            "|  2|Bobby|   Punjab|\n",
            "+---+-----+---------+\n",
            "\n",
            "FULL OUTER JOIN\n",
            "+---+-----+---------+\n",
            "| id| name|     city|\n",
            "+---+-----+---------+\n",
            "|  1| Amay|New Delhi|\n",
            "|  2|Bobby|   Punjab|\n",
            "|  3|Charu|     null|\n",
            "|  4| null|  Haryana|\n",
            "+---+-----+---------+\n",
            "\n",
            "LEFT SEMI JOIN\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1| Amay|\n",
            "|  2|Bobby|\n",
            "+---+-----+\n",
            "\n",
            "LEFT ANTI JOIN\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  3|Charu|\n",
            "+---+-----+\n",
            "\n",
            "CROSS JOIN\n",
            "+---+-----+---+---------+\n",
            "| id| name| id|     city|\n",
            "+---+-----+---+---------+\n",
            "|  1| Amay|  1|New Delhi|\n",
            "|  1| Amay|  2|   Punjab|\n",
            "|  1| Amay|  4|  Haryana|\n",
            "|  2|Bobby|  1|New Delhi|\n",
            "|  3|Charu|  1|New Delhi|\n",
            "|  2|Bobby|  2|   Punjab|\n",
            "|  2|Bobby|  4|  Haryana|\n",
            "|  3|Charu|  2|   Punjab|\n",
            "|  3|Charu|  4|  Haryana|\n",
            "+---+-----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK**\n",
        "\n",
        "Data :\n",
        "\n",
        "df1 = spark.createDataFrame([(23, 'Aarav'), (42, 'Daisy'), (37, 'Avi'),(43, 'Abir'),(11, 'Raghav'),(19, 'Nitara')], ['roll_no', 'student_name'])\n",
        "print('df1')\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame([(42, 'English'), (37, 'Maths'), (19, 'Computer Science'),(8, 'Sociology'),(16, 'IT'),(23, 'Hindi')], ['roll_no', 'Sanskrit'])\n",
        "print('df2')\n",
        "df2.show()\n",
        "\n",
        "- Perform inner, right, left, cross, left semi, left anti, outer join on the given datasets."
      ],
      "metadata": {
        "id": "xe4nbF7l_J9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Text Split:**\n",
        "\n",
        "   - Explanation: Text split is used to split a column into multiple columns based on a delimiter.It  is used to split a string column into an array of substrings based on a delimiter.\n",
        "\n",
        "   - Syntax: `split(column, delimiter)`\n",
        "\n",
        "   delimiter is the character or string used to split the column\n",
        "\n",
        "   - Usage: Useful when you want to extract specific parts from a column containing text.\n",
        "\n",
        "   Code Example:\n",
        "   ```python\n",
        "   df = df.withColumn('new_column', split(df['text_column'], ','))\n",
        "   ```\n",
        "\n",
        "   - Task:\n",
        "     Split a column named `name` into two columns: `first_name` and `last_name`.\n"
      ],
      "metadata": {
        "id": "XzA0VhxLBkyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "\n",
        "df = spark.createDataFrame([(1, 'Amay Gupta'), (2, 'Deepak Kaushik'), (3, 'Charu Ahuja')], ['id', 'name'])\n",
        "\n",
        "print('Original df')\n",
        "df.show()\n",
        "\n",
        "df_split = df.withColumn('first_name', split(df['name'], ' ')[0])\\\n",
        ".withColumn('last_name', split(df['name'], ' ')[1])\n",
        "\n",
        "print('After split')\n",
        "df_split.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr9RiOQnCbqp",
        "outputId": "34a37bbd-14cd-4f23-e690-11f7160f9a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df\n",
            "+---+--------------+\n",
            "| id|          name|\n",
            "+---+--------------+\n",
            "|  1|    Amay Gupta|\n",
            "|  2|Deepak Kaushik|\n",
            "|  3|   Charu Ahuja|\n",
            "+---+--------------+\n",
            "\n",
            "After split\n",
            "+---+--------------+----------+---------+\n",
            "| id|          name|first_name|last_name|\n",
            "+---+--------------+----------+---------+\n",
            "|  1|    Amay Gupta|      Amay|    Gupta|\n",
            "|  2|Deepak Kaushik|    Deepak|  Kaushik|\n",
            "|  3|   Charu Ahuja|     Charu|    Ahuja|\n",
            "+---+--------------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "Create a dataframe with 2 rows and 1 column(city,state) and split it into 2 columns city and state."
      ],
      "metadata": {
        "id": "3EEZlLAREKXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Text Length:\n",
        "- Explanation: The length function is used to calculate the length of a string column.\n",
        "\n",
        "- Syntax:\n",
        "```length_col = F.length(column_name)```\n",
        "\n",
        "- Usage: column_name is the name of the column for which the length is calculated.\n"
      ],
      "metadata": {
        "id": "cQTUgWh2E-qC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Creating a DataFrame with a string column\n",
        "df = spark.createDataFrame([('Hello, World!',)], ['message'])\n",
        "\n",
        "\n",
        "# Calculating the length of the message column\n",
        "length_col = F.length(df['message'])\n",
        "df.withColumn('message_length', length_col).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eThyw19vFwKr",
        "outputId": "da28173f-eced-4d42-af45-c593365bcd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|      message|message_length|\n",
            "+-------------+--------------+\n",
            "|Hello, World!|            13|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Regex Replace::\n",
        "- Explanation: The regexp_replace function is used to replace substrings in a string column based on a regular expression pattern.\n",
        "\n",
        "- Syntax:\n",
        "```replaced_col = F.regexp_replace(column_name, pattern, replacement)```\n",
        "\n",
        "- column_name is the name of the column to search and replace.\n",
        "- pattern is the regular expression pattern to search for.\n",
        "- replacement is the string to replace the matching pattern with.\n",
        "\n"
      ],
      "metadata": {
        "id": "kJ4BNP_gHmv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Creating a DataFrame\n",
        "school = [(1,\"DPS\",\"Karol bagh\"),\n",
        "    (2,\"BBPS\",\"Pitampura\"),\n",
        "    (3,\"DAV\",\"Mandir marg\")]\n",
        "df =spark.createDataFrame(school,[\"id\",\"School\",\"Address\"])\n",
        "df.show()\n",
        "\n",
        "\n",
        "#Replace part of string with another string\n",
        "df.withColumn('school', regexp_replace('school', 'DPS', 'Delhi Public School')) \\\n",
        "  .show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVG3F-PVIIK9",
        "outputId": "05bd8441-4f37-4158-bf89-6ac5be0e6efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----------+\n",
            "| id|School|    Address|\n",
            "+---+------+-----------+\n",
            "|  1|   DPS| Karol bagh|\n",
            "|  2|  BBPS|  Pitampura|\n",
            "|  3|   DAV|Mandir marg|\n",
            "+---+------+-----------+\n",
            "\n",
            "+---+-------------------+-----------+\n",
            "| id|             school|    Address|\n",
            "+---+-------------------+-----------+\n",
            "|  1|Delhi Public School| Karol bagh|\n",
            "|  2|               BBPS|  Pitampura|\n",
            "|  3|                DAV|Mandir marg|\n",
            "+---+-------------------+-----------+\n",
            "\n",
            "+---+------+-----------+\n",
            "| id|school|    Address|\n",
            "+---+------+-----------+\n",
            "|  1|   DPS| Karol bagh|\n",
            "|  2|  BBPS|  Pitampura|\n",
            "|  3|   DAV|Mandir marg|\n",
            "+---+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Remove whitespaces/ MoveSpace:\n",
        "- Explanation: The trim function is used to remove leading and trailing whitespace from a string column.\n",
        "\n",
        "```trimmed_col = F.trim(column_name)```\n"
      ],
      "metadata": {
        "id": "aIMii1W4M0-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Creating a DataFrame with a string column\n",
        "data=[('     Hello, World!    ',)]\n",
        "df = spark.createDataFrame(data, ['message'])\n",
        "df.show()\n",
        "\n",
        "# Removing leading and trailing whitespace from the message column\n",
        "trimmed_col = F.trim(df['message'])\n",
        "df.withColumn('trimmed_message', trimmed_col).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAxcOAaIM7eP",
        "outputId": "1e0322c0-1376-4499-8dbe-373d1923ab84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|             message|\n",
            "+--------------------+\n",
            "|     Hello, World...|\n",
            "+--------------------+\n",
            "\n",
            "+--------------------+---------------+\n",
            "|             message|trimmed_message|\n",
            "+--------------------+---------------+\n",
            "|     Hello, World...|  Hello, World!|\n",
            "+--------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try ltrim() and rtrim() on your own."
      ],
      "metadata": {
        "id": "JAuxNACPmZqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Casting:\n",
        "- Explanation: The cast function is used to convert a column to a different data type.\n",
        "\n",
        "- Syntax:\n",
        "```cast_col = column_name.cast(data_type)```\n",
        "\n",
        "- Usage:\n",
        "column_name is the name of the column to cast.\n",
        "data_type is the desired data type to convert the column to."
      ],
      "metadata": {
        "id": "il6AdUzePteH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Creating a DataFrame with a string column\n",
        "df = spark.createDataFrame([('42',)], ['number'])\n",
        "print('Before casting')\n",
        "df.printSchema()\n",
        "df.show()\n",
        "\n",
        "# Casting the number column to integer type\n",
        "df=df.withColumn('number',df.number.cast(IntegerType()))\n",
        "print('After casting')\n",
        "df.show()\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D90fERV5Ohb1",
        "outputId": "45f110a3-7ed3-47cb-d695-7e8a5a1f889f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before casting\n",
            "root\n",
            " |-- number: string (nullable = true)\n",
            "\n",
            "+------+\n",
            "|number|\n",
            "+------+\n",
            "|    42|\n",
            "+------+\n",
            "\n",
            "After casting\n",
            "+------+\n",
            "|number|\n",
            "+------+\n",
            "|    42|\n",
            "+------+\n",
            "\n",
            "root\n",
            " |-- number: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. to_date:\n",
        "- Explanation:\n",
        "\n",
        "to_date() function is used to convert String to Date format of a DataFrame column.\n",
        "\n",
        "to_date()  function is used to format string (StringType) to date (DateType) column.\n",
        "\n",
        "- Syntax\n",
        "to_date(column,format)\n",
        "Example: to_date(col(\"string_column\"),\"MM-dd-yyyy\")\n",
        "\n",
        "- Usage:\n",
        "\n",
        "This function takes the first argument as a date string and the second argument takes the pattern the date is in the first argument.\n",
        "\n"
      ],
      "metadata": {
        "id": "O4pyKussP_hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df=spark.createDataFrame([[\"02-03-2013\"],[\"05-06-2023\"]],[\"input\"])\n",
        "df.printSchema()\n",
        "\n",
        "df= df.select(col(\"input\"),to_date(col(\"input\"),\"MM-dd-yyyy\").alias(\"date\"))\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "df1=spark.createDataFrame(\n",
        "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
        "        schema=[\"id\",\"input_timestamp\"])\n",
        "df1.show()\n",
        "df1.printSchema()\n",
        "\n",
        "#Timestamp String to DateType\n",
        "df1.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
        "  .show()\n",
        "\n",
        "#Timestamp Type to DateType\n",
        "df1.withColumn(\"today's date\",to_date(current_timestamp())) \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TKp8X_6QUFU",
        "outputId": "53c3877c-2682-429f-ebd9-406b01039fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- input: string (nullable = true)\n",
            "\n",
            "+----------+----------+\n",
            "|     input|      date|\n",
            "+----------+----------+\n",
            "|02-03-2013|2013-02-03|\n",
            "|05-06-2023|2023-05-06|\n",
            "+----------+----------+\n",
            "\n",
            "root\n",
            " |-- input: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            "\n",
            "+---+--------------------+\n",
            "| id|     input_timestamp|\n",
            "+---+--------------------+\n",
            "|  1|2019-06-24 12:01:...|\n",
            "+---+--------------------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- input_timestamp: string (nullable = true)\n",
            "\n",
            "+---+--------------------+----------+\n",
            "| id|     input_timestamp| date_type|\n",
            "+---+--------------------+----------+\n",
            "|  1|2019-06-24 12:01:...|2019-06-24|\n",
            "+---+--------------------+----------+\n",
            "\n",
            "+---+--------------------+------------+\n",
            "| id|     input_timestamp|today's date|\n",
            "+---+--------------------+------------+\n",
            "|  1|2019-06-24 12:01:...|  2023-06-21|\n",
            "+---+--------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Concatenation of Two Columns:\n",
        "- Explanation: Concatenation is used to combine two or more columns into a single column.\n",
        "-  Syntax: `concat(column1, column2, ...)`\n",
        "-  Usage: Useful when you need to merge the values of multiple columns into one.\n",
        "\n",
        "```   df = df.withColumn('new_column', concat(df['column1'], df['column2']))```"
      ],
      "metadata": {
        "id": "Eo6mKx7aFdLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sqHK1k69Uau_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat,col\n",
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "print('Original df')\n",
        "df.show()\n",
        "\n",
        "df2=df.select(concat(df.firstname,df.middlename,df.lastname)\n",
        "              .alias(\"FullName\"),\"dob\",\"gender\",\"salary\")\n",
        "df2.show()\n",
        "\n",
        "df3=df.select(concat_ws('_',df.firstname,df.middlename,df.lastname)\n",
        "              .alias(\"FullName\"),\"dob\",\"gender\",\"salary\")\n",
        "df3.show()\n",
        "\n",
        "# concat_ws('delimiter',df.col_name1,df.col_name2).alias('new_col_name')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bflJgcQcUXPQ",
        "outputId": "e8002aa8-b8bb-42eb-c963-beca7cde71f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n",
            "+--------------+----------+------+------+\n",
            "|      FullName|       dob|gender|salary|\n",
            "+--------------+----------+------+------+\n",
            "|    JamesSmith|1991-04-01|     M|  3000|\n",
            "|   MichaelRose|2000-05-19|     M|  4000|\n",
            "|RobertWilliams|1978-09-05|     M|  4000|\n",
            "|MariaAnneJones|1967-12-01|     F|  4000|\n",
            "|  JenMaryBrown|1980-02-17|     F|    -1|\n",
            "+--------------+----------+------+------+\n",
            "\n",
            "+----------------+----------+------+------+\n",
            "|        FullName|       dob|gender|salary|\n",
            "+----------------+----------+------+------+\n",
            "|    James__Smith|1991-04-01|     M|  3000|\n",
            "|   Michael_Rose_|2000-05-19|     M|  4000|\n",
            "|Robert__Williams|1978-09-05|     M|  4000|\n",
            "|Maria_Anne_Jones|1967-12-01|     F|  4000|\n",
            "|  Jen_Mary_Brown|1980-02-17|     F|    -1|\n",
            "+----------------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark string functions you need to practice are as given below with their explanation and syntax:**\n",
        "\n",
        "\n",
        "*   contains() - It checks whether a string column contains a specified substring or not.\n",
        "\n",
        "```df.filter(col('column_name').contains('substring'))```\n",
        "\n",
        "*   startswith() - It checks whether a string column starts with a specified substring or not.\n",
        "\n",
        "```df.filter(col('column_name').startswith('substring'))```\n",
        "\n",
        "* endswith()- It checks whether a string column ends with a specified substring or not.\n",
        "\n",
        "```df.filter(col('column_name').endswith('substring'))```\n",
        "\n",
        "*  initcap() - It converts the first letter of each word in a string column to uppercase.\n",
        "\n",
        "```df.select(initcap(df.column_name))```\n",
        "\n",
        "* upper() - It converts a string column to uppercase.\n",
        "\n",
        "```df.select(upper(df.column_name))```\n",
        "\n",
        "* lower() - It converts a string column to lowercase.\n",
        "\n",
        "```df.select(lower(df.column_name))```\n",
        "\n",
        "* substring() - It extracts a substring from a string column based on a starting position and length.\n",
        "\n",
        "```df.select(substring('column_name', position, length))```\n",
        "\n",
        "* lpad()- It pads a string column on the left with a specified character to a specified length.\n",
        "\n",
        "```df.select(lpad('column_name', length, 'character'))```\n",
        "\n",
        "* rpad()- It pads a string column on the right with a specified character to a specified length.\n",
        "\n",
        "```df.select(rpad(df.column_name, length, 'character'))```\n",
        "\n",
        "* ltrim() - It removes leading whitespace from a string column.\n",
        "\n",
        "```df.select(ltrim(df.column_name))```\n",
        "\n",
        "* rtrim() - It removes trailing whitespace from a string column.\n",
        "\n",
        "```df.select(rtrim(df.column_name))```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bY-dZ0Phvuuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 4"
      ],
      "metadata": {
        "id": "Jdt1BN575nQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SQL**\n",
        "\n",
        "Spark SQL -  It provides a programming interface for querying structured and semi-structured data using SQL-like syntax. It allows developers to seamlessly integrate SQL queries with their Spark applications.\n",
        "\n",
        "- To create a view from a DataFrame, you can use the createOrReplaceTempView() method. It creates a temporary view that is accessible only within the current SparkSession.\n",
        "\n",
        "```df.createOrReplaceTempView(\"view_name\")```\n",
        "\n",
        "- Once you have created a view, you can run SQL queries on it using the spark.sql() method.\n",
        "\n",
        "- select query\n",
        "\n",
        "```spark.sql(\"SELECT * FROM view_name\")```\n",
        "\n"
      ],
      "metadata": {
        "id": "SMf7GDyo5sJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a scenario where we have a dataset of employees and their respective departments. We will create a DataFrame, register it as a temporary view, and perform SQL queries using Spark SQL."
      ],
      "metadata": {
        "id": "i-JBxlXhJ1u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CREATE TABLE COMPANY(\n",
        "   ID INT PRIMARY KEY     NOT NULL,\n",
        "   NAME           TEXT    NOT NULL,\n",
        "   AGE            INT     NOT NULL,\n",
        "   ADDRESS        CHAR(50),\n",
        "   SALARY         REAL,\n",
        "   JOIN_DATE\t  DATE\n",
        ");\n",
        "\n",
        "\n",
        "- INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) VALUES (1, 'Amay', 32, 'New Delhi', 20000.00,'2022-07-13')\n",
        ",(2, 'Akshay', 22, 'Uttar Pradesh', 40000.00,'2023-09-18')\n",
        ",(3, 'Nishtha', 21, 'Haryana', 50000.00,'2021-11-28')\n",
        ",(4, 'Nysha', 18, 'New Delhi', 20000.00,'2022-07-13');\n",
        "\n",
        "\n",
        "- SELECT id, name, age, address, salary, join_date FROM public.company;"
      ],
      "metadata": {
        "id": "cSdVmLmybyTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SQL\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame from a list of tuples\n",
        "data = [(\"Amay\", \"Engineering\", 5000),\n",
        "        (\"Bhawna\", \"Sales\", 3000),\n",
        "        (\"Charu\", \"Engineering\", 4500),\n",
        "        (\"Daisy\", \"Sales\", 3500)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Department\", \"Salary\"])\n",
        "print('Original df')\n",
        "df.show()\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "# Run SQL queries on the view\n",
        "\n",
        "print('After running select query ')\n",
        "spark.sql(\"SELECT * FROM employees\").show()\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM employees WHERE Salary > 4000\")\n",
        "print('On running SQL query with salary > 4000 condition')\n",
        "result.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaJUrbI8WTkp",
        "outputId": "65a5099c-fc3b-4929-b4e6-80546b1d30a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df\n",
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|  Amay|Engineering|  5000|\n",
            "|Bhawna|      Sales|  3000|\n",
            "| Charu|Engineering|  4500|\n",
            "| Daisy|      Sales|  3500|\n",
            "+------+-----------+------+\n",
            "\n",
            "After running select query \n",
            "+------+-----------+------+\n",
            "|  Name| Department|Salary|\n",
            "+------+-----------+------+\n",
            "|  Amay|Engineering|  5000|\n",
            "|Bhawna|      Sales|  3000|\n",
            "| Charu|Engineering|  4500|\n",
            "| Daisy|      Sales|  3500|\n",
            "+------+-----------+------+\n",
            "\n",
            "On running SQL query with salary > 4000 condition\n",
            "+-----+-----------+------+\n",
            "| Name| Department|Salary|\n",
            "+-----+-----------+------+\n",
            "| Amay|Engineering|  5000|\n",
            "|Charu|Engineering|  4500|\n",
            "+-----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using groupBy query:\n"
      ],
      "metadata": {
        "id": "iKTaf_-3MCnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('After Group by query ')\n",
        "result = spark.sql(\"SELECT Department, AVG(Salary) FROM employees GROUP BY Department\")\n",
        "result.show()\n",
        "\n",
        "result = spark.sql(\"SELECT Department, AVG(Salary) FROM employees GROUP BY Department\")\n",
        "result.show()\n",
        "\n",
        "resul= spark.sql(\"SELECT Department, COUNT(Salary) FROM employees GROUP BY Department\")\n",
        "resul.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Qo-RdtMSYB",
        "outputId": "a584be05-9923-4973-d4f8-ab45ffdacbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Group by query \n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|     3250.0|\n",
            "|Engineering|     4750.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|     3250.0|\n",
            "|Engineering|     4750.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "+-----------+-------------+\n",
            "| Department|count(Salary)|\n",
            "+-----------+-------------+\n",
            "|      Sales|            2|\n",
            "|Engineering|            2|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Joins:\n",
        "- Spark SQL supports different types of joins, such as inner join, outer join, left join, right join, etc."
      ],
      "metadata": {
        "id": "cc7iKpy8NiIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.createDataFrame([(1, 'Amay'), (2, 'Bobby'), (3, 'Charu')], ['id', 'name'])\n",
        "print('df1')\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame([(1, 'New Delhi'), (2, 'Punjab'), (4, 'Haryana')], ['id', 'city'])\n",
        "print('df2')\n",
        "df2.show()\n",
        "\n",
        "df1.createOrReplaceTempView(\"left_table\")\n",
        "df2.createOrReplaceTempView(\"right_table\")\n",
        "\n",
        "result = spark.sql(\"SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id\")\n",
        "print('After join')\n",
        "result.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJNJtxENqoK",
        "outputId": "82952d23-553f-4446-9b19-d9be1f1b730b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df1\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1| Amay|\n",
            "|  2|Bobby|\n",
            "|  3|Charu|\n",
            "+---+-----+\n",
            "\n",
            "df2\n",
            "+---+---------+\n",
            "| id|     city|\n",
            "+---+---------+\n",
            "|  1|New Delhi|\n",
            "|  2|   Punjab|\n",
            "|  4|  Haryana|\n",
            "+---+---------+\n",
            "\n",
            "After join\n",
            "+---+-----+---+---------+\n",
            "| id| name| id|     city|\n",
            "+---+-----+---+---------+\n",
            "|  1| Amay|  1|New Delhi|\n",
            "|  2|Bobby|  2|   Punjab|\n",
            "+---+-----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data1 = [\n",
        "    (\"1\", \"Rahul\", \"Kumar\", \"Delhi\"),\n",
        "    (\"2\", \"Priya\", \"Sharma\", \"Mumbai\"),\n",
        "    (\"3\", \"Amit\", \"Verma\", \"Kolkata\"),\n",
        "    (\"4\", \"Sneha\", \"Gupta\", \"Chennai\"),\n",
        "    (\"5\", \"Vikram\", \"Singh\", \"Bangalore\"),\n",
        "    (\"6\", \"Neha\", \"Patel\", \"Hyderabad\"),\n",
        "    (\"7\", \"Rajesh\", \"Yadav\", \"Pune\"),\n",
        "    (\"8\", \"Sara\", \"Malhotra\", \"Jaipur\"),\n",
        "    (\"9\", \"Rakesh\", \"Shah\", \"Ahmedabad\"),\n",
        "    (\"10\", \"Anita\", \"Rao\", \"Chandigarh\")\n",
        "]\n",
        "\n",
        "df1 = spark.createDataFrame(data1, [\"id\", \"first_name\", \"last_name\", \"city\"])\n",
        "df1.show()\n",
        "\n",
        "data2 = [\n",
        "    (\"1\", \"Rahul\", \"Kumar\", \"IT\"),\n",
        "    (\"2\", \"Priya\", \"Sharma\", \"Finance\"),\n",
        "    (\"3\", \"Amit\", \"Verma\", \"Marketing\"),\n",
        "    (\"4\", \"Sneha\", \"Gupta\", \"HR\"),\n",
        "    (\"5\", \"Vikram\", \"Singh\", \"IT\"),\n",
        "    (\"6\", \"Neha\", \"Patel\", \"Finance\"),\n",
        "    (\"7\", \"Rajesh\", \"Yadav\", \"Marketing\"),\n",
        "    (\"8\", \"Sara\", \"Malhotra\", \"HR\"),\n",
        "    (\"9\", \"Rakesh\", \"Shah\", \"IT\"),\n",
        "    (\"10\", \"Anita\", \"Rao\", \"Finance\")\n",
        "]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, [\"id\", \"first_name\", \"last_name\", \"department\"])\n",
        "df2.show()\n",
        "\n",
        "Perform select, group by , where and join query on it."
      ],
      "metadata": {
        "id": "2aKO-3FUOea2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting PySpark with PostgreSQL:\n",
        "\n",
        "- PySpark provides support for connecting to external databases like PostgreSQL to read data from tables and write data back to tables. By establishing a connection, you can leverage the power of PySpark's distributed processing capabilities on your PostgreSQL data.\n",
        "\n",
        "- To connect PySpark with PostgreSQL, you need to provide the necessary configurations."
      ],
      "metadata": {
        "id": "spV2mTKpQcm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Connecting PySpark with PostgreSQL:\n",
        "- To connect PySpark with PostgreSQL, you need to provide the appropriate JDBC URL, along with the username and password. Ensure that you have the PostgreSQL JDBC driver (JAR file) available and specified in the SparkSession configuration. Replace the placeholders in the code example with the actual connection details.\n",
        "\n",
        "2. Reading a Table from PostgreSQL:\n",
        "- To read a table from PostgreSQL, use the read.jdbc() method and specify the JDBC URL, table name, and connection properties (username and password). This will create a DataFrame containing the data from the specified table.\n",
        "\n",
        "3. Writing a Table to PostgreSQL:\n",
        "- To write a DataFrame to a PostgreSQL table, use the write.jdbc() method and specify the JDBC URL, table name, mode (e.g., \"append\" or \"overwrite\"), and connection properties. This will write the DataFrame data to the specified table in the PostgreSQL database.\n",
        "\n",
        "- Note: Ensure that you have the necessary access rights and privileges to the PostgreSQL database and tables for successful read and write operations."
      ],
      "metadata": {
        "id": "EUPghjRiRVIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SYNTAX"
      ],
      "metadata": {
        "id": "hW6SOXbZQ6E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQLExample\") \\\n",
        "    .config(\"spark.jars\", \"/path/to/postgresql.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Establish a connection to PostgreSQL\n",
        "url = \"jdbc:postgresql://hostname:port/database\"\n",
        "properties = {\n",
        "    \"user\": \"username\",\n",
        "    \"password\": \"password\"\n",
        "}\n",
        "\n",
        "# Read data from a PostgreSQL table\n",
        "df = spark.read.jdbc(url=url, table=\"table_name\", properties=properties)\n",
        "\n",
        "# Write data to a PostgreSQL table\n",
        "df.write.jdbc(url=url, table=\"table_name\", mode=\"append\", properties=properties)\n"
      ],
      "metadata": {
        "id": "EioJxEJ_QnZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE :"
      ],
      "metadata": {
        "id": "KZ-MBwAfQ8T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQLExample\") \\\n",
        "    .config(\"spark.jars\", \"/path/to/postgresql.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Establish a connection to PostgreSQL\n",
        "url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
        "properties = {\n",
        "    \"user\": \"myusername\",\n",
        "    \"password\": \"mypassword\"\n",
        "}\n",
        "\n",
        "# Read data from a PostgreSQL table\n",
        "df = spark.read.jdbc(url=url, table=\"source_table\", properties=properties)\n",
        "\n",
        "# Perform transformations on the DataFrame\n",
        "transformed_df = df.filter(df[\"age\"] > 30).select(\"name\", \"age\")\n",
        "\n",
        "# Write data to a PostgreSQL table\n",
        "transformed_df.write.jdbc(url=url, table=\"target_table\", mode=\"append\", properties=properties)\n"
      ],
      "metadata": {
        "id": "R3sq2ztCQ2wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* INCREMENTAL LOADING\n",
        "\n",
        "- Incremental loading is a technique used in Extract, Transform, Load (ETL) projects to update a target dataset with only the new or changed data since the last load.\n",
        "\n",
        "- Instead of reloading the entire dataset, incremental loading focuses on processing and appending the delta or changes to the existing dataset, thereby reducing processing time and resource usage.\n",
        "\n",
        "- It is useful when dealing with large datasets or scenarios where data is continuously updated. By identifying and processing only the incremental changes, the ETL process becomes more efficient and faster.\n"
      ],
      "metadata": {
        "id": "ahgDbpg6S2F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntax**\n",
        "\n",
        "The general approach involves comparing timestamps or other indicators to identify new or changed data"
      ],
      "metadata": {
        "id": "8YFfFqe3Tjc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Loading Existing Data:\n",
        "Load the existing data from the target dataset using the appropriate method, such as spark.read.parquet(), to read the data from an existing Parquet file.\n",
        "\n",
        "- Loading New Data:\n",
        "Load the new data from the source system, such as a database table, using the appropriate method, such as spark.read.jdbc(), to fetch the data.\n",
        "\n",
        "- Identifying Incremental Changes:\n",
        "Filter the new data based on the desired condition (e.g., data that is newer than a specific date or timestamp) to identify the incremental changes.\n",
        "\n",
        "- Combining Data:\n",
        "Combine the existing data with the incremental data using DataFrame operations like union() to create a new DataFrame that contains both datasets.\n",
        "\n",
        "- Writing to Parquet:\n",
        "Write the combined data to a new Parquet file using the write.mode(\"overwrite\").parquet() method, specifying the desired save location and file format."
      ],
      "metadata": {
        "id": "N7PW1_88Ugmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load existing data\n",
        "existing_data = spark.read.parquet(\"existing_data.parquet\")\n",
        "\n",
        "# Load new data\n",
        "new_data = spark.read.jdbc(url=url, table=\"source_table\", properties=properties)\n",
        "\n",
        "# Identify the incremental changes\n",
        "incremental_data = new_data.filter(new_data[\"date\"] >= start_date)\n",
        "\n",
        "# Combine existing and incremental data\n",
        "combined_data = existing_data.union(incremental_data)\n",
        "\n",
        "# Write the combined data to a new Parquet file\n",
        "combined_data.write.mode(\"overwrite\").parquet(\"updated_data.parquet\")\n"
      ],
      "metadata": {
        "id": "d831iNSlTu61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40sgJHbFOdSo",
        "outputId": "0bb51013-6b0a-4d3f-fd8b-f3e3a283b066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load existing data\n",
        "existing_data = spark.read.csv(\"/content/drive/MyDrive/Sample_Data/inventory.csv\",header=True,inferSchema=True)\n",
        "existing_data.show()\n",
        "print('COUNT ',existing_data.count())\n",
        "\n",
        "\n",
        "# new_data= spark.read.csv(\"/content/drive/MyDrive/Sample_Data/Inventory_2.csv\",header=True,inferSchema=True)\n",
        "# print('COUNT ',new_data.count())\n",
        "# new_data.show()\n",
        "\n",
        "# incremental_data = new_data.filter(new_data[\"PostingDate\"] >= '2023-05-01')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLDzYSLUVdZ4",
        "outputId": "1ac07408-bebf-40eb-88ad-71bf4c56805c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-------------------+---------+--------------+------------+--------+-----------+----------+-----------+---------------+---------+----------+---------------------+\n",
            "|EntryNo_| ItemNo_|        PostingDate|EntryType|   DocumentNo_|LocationCode|Quantity|VariantCode| SerialNo_|QuantityLCY|LinkDocumentNo_|YearMonth|DocumentNo|last_day(PostingDate)|\n",
            "+--------+--------+-------------------+---------+--------------+------------+--------+-----------+----------+-----------+---------------+---------+----------+---------------------+\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2019-09-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "| 3250000|IT002333|2020-04-30 00:00:00|        1|DC6202021-0013|     CHN-TRA|    -1.0| 864279-B21|F200-CMD19|       -1.0| DC6202021-0013|    20204|      null|  2020-04-30 00:00:00|\n",
            "+--------+--------+-------------------+---------+--------------+------------+--------+-----------+----------+-----------+---------------+---------+----------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "COUNT  62500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 5"
      ],
      "metadata": {
        "id": "drtnrSkNEvGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hands-on :\n",
        "1. PostgreSQL and pyspark connectivity\n",
        "2. MSSQL and pyspark connectivity"
      ],
      "metadata": {
        "id": "E1AnpdheJt9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PostgreSQL and pyspark connectivity"
      ],
      "metadata": {
        "id": "nyq296BLcowz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PostgreSQL JDBC Connection\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"content/drive/MyDrive/Colab Notebooks/postgresql-42.6.0.jar\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaH_atL9cGLD",
        "outputId": "08cc1343-aab3-4785-a731-678757ae05f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PostgresDbInfo:\n",
        "    Host = \"host_name\"\n",
        "    Port = \"5432\"\n",
        "    PostgresDB = \"db_name\"\n",
        "    PostgresUrl = \"jdbc:postgresql://\" + Host + \"/\" + PostgresDB\n",
        "    props = {\"user\":\"user\", \"password\":\"password\", \"driver\": \"org.postgresql.Driver\"}\n",
        "\n",
        "Query=\"(SELECT * from Access Control) as df\"\n",
        "\n",
        "postgres_df = spark.read.format(\"jdbc\").options(url=PostgresDbInfo.PostgresUrl, dbtable=Query,user=PostgresDbInfo.props[\"user\"],password=PostgresDbInfo.props[\"password\"],driver= PostgresDbInfo.props[\"driver\"]).load()\n",
        "postgres_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "UyleqAPqcSzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. MSSQL and pyspark connectivity"
      ],
      "metadata": {
        "id": "OBoPdkA4cruG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MSSQL JDBC Connection\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"/content/drive/MyDrive/Colab Notebooks/mssql-jdbc-12.2.0.jre8.jar\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "4-QUXDSScvXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e408b4a-26f2-46cd-aca0-8f0d073b6b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectionInfo:\n",
        "    JDBC_PARAM = \"jdbc\"\n",
        "    SQL_SERVER_DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "    SQL_URL=\"jdbc:sqlserver://4.240.11.180:1433;databaseName=Parijat;user=SparkTraining;password=SparkTraining@123;encrypt=true;trustServerCertificate=true\"\n",
        "\n",
        "\n",
        "query = \"(SELECT * from [Student_incremental]) as data\"\n",
        "viewDF = spark.read.format(ConnectionInfo.JDBC_PARAM).options(url=ConnectionInfo.SQL_URL,dbtable=query,driver=ConnectionInfo.SQL_SERVER_DRIVER).load()\n",
        "viewDF.show()"
      ],
      "metadata": {
        "id": "Afr_AlKndOp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8673054e-160e-41d9-9b1b-5ad158316f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-------------------+-----------+---+\n",
            "| ID|            Name|             InDtTm|       Date|Age|\n",
            "+---+----------------+-------------------+-----------+---+\n",
            "|  1|   ABHISHEK 1234|2023-05-16 12:08:47|2023-03-30 | 26|\n",
            "|  2|   AMIT kumar 32|2023-05-29 13:51:46|2023-03-31 | 24|\n",
            "|  3|naveen gulati 32|2023-05-29 13:51:46|2023-03-01 | 25|\n",
            "|  4|            hary|2023-05-16 15:19:26|2023-03-02 | 27|\n",
            "|  5|           mansu|2023-05-16 12:44:08|2023-03-03 | 28|\n",
            "|  6|           sagar|2023-05-15 14:15:28|2023-03-04 | 21|\n",
            "|  7|          satyam|2023-05-15 14:15:28|2023-06-05 | 22|\n",
            "|  8|           ayush|2023-05-15 14:15:28|2023-06-06 | 27|\n",
            "|  9|          akshay|2023-05-15 14:15:28|2023-06-07 | 28|\n",
            "| 10|           dhruv|2023-05-15 14:15:28|2023-06-08 | 29|\n",
            "| 11|          ankush|2023-05-15 14:15:28|2023-06-09 | 20|\n",
            "| 12|         abhinav|2023-05-15 14:15:28|2023-06-10 | 23|\n",
            "| 13|          mayank|2023-05-15 14:15:28|2023-03-29 | 19|\n",
            "| 14|           hruva|2023-05-15 14:15:28|2023-03-28 | 17|\n",
            "| 15|            Maya|2023-05-15 14:15:28|2023-03-27 | 23|\n",
            "| 16|           navin|2023-05-15 14:15:28|2023-06-01 | 25|\n",
            "| 17|              BA|2023-05-15 14:15:28|2023-03-04 | 25|\n",
            "| 18|             ABC|2023-05-15 14:15:28|2023-06-11 | 15|\n",
            "| 19|             BCA|2023-05-15 14:15:28|2023-05-11 | 15|\n",
            "| 28|    abhinav test|2023-05-15 14:15:28|2023-05-10 | 28|\n",
            "+---+----------------+-------------------+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**\n",
        "\n",
        "- Read the data from the shared csv file.\n",
        "- Write it to the SQL database.\n"
      ],
      "metadata": {
        "id": "e1lwKV9Adm5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**\n",
        "\n",
        "- Add some rows to the table in SQL using SQL commands.\n",
        "- Now Read the table from SQL database and check the updated data.\n"
      ],
      "metadata": {
        "id": "3vYAVuCmeJvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3**\n",
        "\n",
        "- Create 2 dataframes on your own having difference in columns and merge them."
      ],
      "metadata": {
        "id": "kWxfhHX9exWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4**\n",
        "\n",
        "1. Use Case1 : Create a SparkSession and Load Data\n",
        "\n",
        "- Create a SparkSession named \"SparkSessionAssignment\".\n",
        "- Load a CSV file named \"data.csv\" into a DataFrame named \"df\".\n",
        "\n",
        "2. Use Case: Select Columns\n",
        "\n",
        "- Select and display only the \"name\" and \"age\" columns from the DataFrame.\n",
        "\n",
        "3. Use Case: Add a New Column\n",
        "\n",
        "- Add a new column named \"age_plus_5\" that contains the age increased by 5 for each row.\n",
        "\n",
        "4. Use Case: Rename Columns\n",
        "\n",
        "- Rename the \"age\" column to \"age_years\" in the DataFrame.\n",
        "\n",
        "5. Use Case: Filter Data\n",
        "\n",
        "- Filter the DataFrame to include only rows where the age is greater than 30.\n",
        "\n",
        "6. Use Case: Group By and Aggregation\n",
        "\n",
        "- Group the DataFrame by the \"city\" column and calculate the average age for each city.\n",
        "\n",
        "7. Use Case: Order By\n",
        "\n",
        "- Sort the DataFrame in descending order based on the \"age_years\" column.\n",
        "\n",
        "8. Use Case: Join DataFrames\n",
        "\n",
        "- Create a second DataFrame named \"df2\" from a CSV file named \"data2.csv\" with columns \"name\" and \"occupation\".\n",
        "\n",
        "- Join(left) \"df\" and \"df2\" on the \"name\" column and display the result.\n",
        "\n",
        "9. Use Case: Pivot Table\n",
        "\n",
        "- Create a pivot table from the DataFrame, with \"city\" as the index, \"occupation\" as the columns, and the average age as the values."
      ],
      "metadata": {
        "id": "D9gLcg9DfIOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "name,age,city\n",
        "John,25,New York\n",
        "Alice,30,San Francisco\n",
        "Bob,35,New York\n",
        "Charlie,40,Los Angeles"
      ],
      "metadata": {
        "id": "JxkW2SconQDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "name,occupation\n",
        "John,Engineer\n",
        "Alice,Manager\n",
        "Bob,Analyst\n",
        "Charlie,Developer"
      ],
      "metadata": {
        "id": "8wUrUp6gnRoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name,age,city\n",
        "John,25,New York\n",
        "Alice,30,San Francisco\n",
        "Bob,35,New York\n",
        "Charlie,40,Los Angeles\n"
      ],
      "metadata": {
        "id": "znsrJ04_mKzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name,occupation\n",
        "John,Engineer\n",
        "Alice,Manager\n",
        "Bob,Analyst\n",
        "Charlie,Developer\n"
      ],
      "metadata": {
        "id": "nBwJqALxmLtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSessionAssignment\").getOrCreate()\n",
        "\n",
        "# Use Case 1: Load Data\n",
        "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Use Case 2: Select Columns\n",
        "df.select(\"name\", \"age\").show()\n",
        "\n",
        "# Use Case 3: Add a New Column\n",
        "df = df.withColumn(\"age_plus_5\", df[\"age\"] + 5)\n",
        "\n",
        "# Use Case 4: Rename Columns\n",
        "df = df.withColumnRenamed(\"age\", \"age_years\")\n",
        "\n",
        "# Use Case 5: Filter Data\n",
        "filtered_df = df.filter(df[\"age_years\"] > 30)\n",
        "\n",
        "# Use Case 6: Group By and Aggregation\n",
        "grouped_df = df.groupBy(\"city\").avg(\"age_years\")\n",
        "\n",
        "# Use Case 7: Order By\n",
        "sorted_df = df.orderBy(df[\"age_years\"].desc())\n",
        "\n",
        "# Use Case 8: Join DataFrames\n",
        "df2 = spark.read.csv(\"data2.csv\", header=True, inferSchema=True)\n",
        "joined_df = df.join(df2, \"name\",\"left\")\n",
        "\n",
        "# Use Case 9: Pivot Table\n",
        "pivot_df = df.groupBy(\"city\").pivot(\"occupation\").avg(\"age_years\")\n",
        "\n",
        "# Display the results\n",
        "filtered_df.show()\n",
        "grouped_df.show()\n",
        "sorted_df.show()\n",
        "joined_df.show()\n",
        "pivot_df.show()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "zpZ9KVyFhUdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 6"
      ],
      "metadata": {
        "id": "Y2GjCrMOjuf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parquet File**\n",
        "- Parquet is a columnar storage file format that is optimized for big data processing.\n",
        "\n",
        "- It provides efficient compression and encoding schemes, allowing for fast data access and column pruning.\n",
        "\n",
        "- Parquet files are designed to be highly efficient for both reading and writing large datasets."
      ],
      "metadata": {
        "id": "EBklQHYMj1iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntax**\n",
        "\n",
        "- To read a Parquet file, you can use the following syntax in different programming languages:\n"
      ],
      "metadata": {
        "id": "Lns7rXzUk79m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# if os.path.exists(\"/content/drive/MyDrive/Sample_Data/practice_file.parquet\"):\n",
        "#   print(\"yes\")\n",
        "# else:\n",
        "#   print(\"no\")\n",
        "\n",
        "# fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "\n",
        "#     if fs.exists(sc._jvm.org.apache.hadoop.fs.Path(\"destination\"))\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read Parquet file\n",
        "df = spark.read.parquet(\"/content/drive/MyDrive/Sample_Data/practice_file.parquet\")\n",
        "\n",
        "# Show the contents of the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-6wrI8lJf4",
        "outputId": "b2548d1e-5fdc-498f-feaf-bcb57049695b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "+---------+----------+--------+-----+------+------+------------+\n",
            "|firstname|middlename|lastname|   id|gender|salary|firstnameLen|\n",
            "+---------+----------+--------+-----+------+------+------------+\n",
            "|   naveen|          |Williams|42114|     M|  4000|           6|\n",
            "|    mansi|      Anne|   Jones|39192|     F|  4000|           5|\n",
            "|   harish|      Mary|   Brown|     |     F|    -1|           6|\n",
            "+---------+----------+--------+-----+------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How to read CSV files and save them as Parquet files using 'append' and 'overwrite' mode  in PySpark:"
      ],
      "metadata": {
        "id": "jymFsYRol7-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV file\n",
        "df_csv = spark.read.csv(\"path/to/csv_file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Write DataFrame to Parquet file (Overwrite mode)\n",
        "df_csv.write.mode(\"overwrite\").parquet(\"path/to/output_file.parquet\")\n",
        "\n",
        "# Write DataFrame to Parquet file (Append mode)\n",
        "df_csv.write.mode(\"append\").parquet(\"path/to/output_file.parquet\")\n"
      ],
      "metadata": {
        "id": "O0_EbOgum5yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Parquet files store data in a compressed and columnar format, which allows for better compression ratios and faster query performance compared to row-based file formats like CSV.\n",
        "\n",
        "Parquet: The standard Parquet file format.\n",
        "- **Parquet with Snappy compression:** Parquet file format with Snappy compression applied to reduce the file size.\n",
        "\n",
        "- **Parquet with Avro schema:** Parquet file format that includes an Avro schema for better data interoperability."
      ],
      "metadata": {
        "id": "wD1gbiCYrNzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compression techniques in Parquet files:**\n",
        "\n",
        "- PySpark supports various compression codecs, including Snappy and Avro. You can specify the compression codec when writing Parquet files\n",
        "(A codec takes data in one form, encodes it into another form and decodes it).\n",
        "\n"
      ],
      "metadata": {
        "id": "p10n0xpanYUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write DataFrame to Parquet file with Snappy compression (Overwrite mode)\n",
        "df_csv.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"path/to/output_file.parquet\")\n",
        "\n",
        "# Write DataFrame to Parquet file with Avro schema and Snappy compression (Overwrite mode)\n",
        "df_csv.write.mode(\"overwrite\").option(\"compression\", \"snappy\").option(\"avroSchema\", \"true\").parquet(\"path/to/output_file.parquet\")\n"
      ],
      "metadata": {
        "id": "3YgK9RBpqMl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partioning**\n",
        "\n",
        "- PySpark partition is a way to split a large dataset into smaller datasets based on one or more partition keys.\n",
        "\n",
        "- When you create a DataFrame from a file/table, based on certain parameters PySpark creates the DataFrame with a certain number of partitions in memory. This is one of the main advantages of PySpark DataFrame over Pandas DataFrame.\n",
        "\n",
        "- Transformations on partitioned data run faster as they execute transformations parallelly for each partition."
      ],
      "metadata": {
        "id": "SzCZOs25sNLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PySpark supports partition in two ways; partition in memory (DataFrame) and partition on the disk (File system).\n",
        "\n",
        "- **Partition in memory**: You can partition or repartition the DataFrame by calling repartition() or coalesce() transformations.\n",
        "\n",
        "- **Partition on disk**: While writing the PySpark DataFrame back to disk, you can choose how to partition the data based on columns using partitionBy() of pyspark.sql.DataFrameWriter.\n"
      ],
      "metadata": {
        "id": "VdKmx4s3sjo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark partitionBy()** is a function of ```pyspark.sql.DataFrameWriter``` class which is used to partition based on column values while writing DataFrame to Disk/File system."
      ],
      "metadata": {
        "id": "4oc9GkIqtUbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Syntax: partitionBy(self, *cols)\n"
      ],
      "metadata": {
        "id": "apvui3QIttFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#partitionBy() multiple columns\n",
        "df.write.option(\"header\",True) \\\n",
        "        .partitionBy(\"col1\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"path\")\n"
      ],
      "metadata": {
        "id": "e-NIeHIMuI85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It creates a folder hierarchy for each partition; we have mentioned the first partition as ```col1``` followed by ```col2``` hence, it creates a col2 folder inside the col1 folder (one folder for each col2 in a col1)."
      ],
      "metadata": {
        "id": "6bkixwuouW8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**maxRecordsPerFile**:\n",
        "\n",
        "- Use option maxRecordsPerFile if you want to control the number of records for each partition. This is particularly helpful when your data is skewed (Having some partitions with very low records and other partitions with high number of records).\n"
      ],
      "metadata": {
        "id": "GsoTzurpvRY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#partitionBy() control number of partitions\n",
        "df.write.option(\"header\",True) \\\n",
        "        .option(\"maxRecordsPerFile\", 2) \\\n",
        "        .partitionBy(\"column_name\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"path\")"
      ],
      "metadata": {
        "id": "qNGZhCppvsYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above example creates multiple part files for each column and each part file contains just 2 records."
      ],
      "metadata": {
        "id": "tcIQUJegv1BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read a Specific Partition**\n",
        "- Reads are much faster on partitioned data. This code snippet retrieves the data from a specific partition \"col1=A and col2=B\". Here, It just reads the data from that specific folder instead of scanning a whole file (when not partitioned)."
      ],
      "metadata": {
        "id": "vYf4n0yov_7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df=spark.read.option(\"header\",True) \\\n",
        "            .csv(\"c:/tmp/zipcodes-col1/col1=A/col2=B\")\n",
        "df.printSchema()\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "WEYPq_xdwKR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark SQL  Read Partition Data**"
      ],
      "metadata": {
        "id": "sdpYs2gCw3gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parqDF = spark.read.option(\"header\",True) \\\n",
        "                  .csv(\"path\")\n",
        "\n",
        "parqDF.createOrReplaceTempView(\"view_name\")\n",
        "spark.sql(\"select * from view_name  where col1='A' and col2 = 'B'\") \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "fn9RFq4Rw586"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partitioning Parquet files on a monthly basis:**"
      ],
      "metadata": {
        "id": "VCFVxV_MxpHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame named df with a date column named \"date_column\"\n",
        "\n",
        "# Import functions\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Add a new column for the month (assuming the date column is of DateType)\n",
        "df = df.withColumn(\"month\", F.date_format(F.col(\"date_column\"), \"yyyy-MM\"))\n",
        "\n",
        "# Write DataFrame to Parquet files partitioned by month\n",
        "df.write.partitionBy(\"month\").parquet(\"path/to/output_directory.parquet\")\n"
      ],
      "metadata": {
        "id": "DdAQ0XWzxpqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using window functions and aggregate functions on Parquet files:**"
      ],
      "metadata": {
        "id": "h3i_omF-x5VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PySpark Window functions are used to calculate results such as the rank, row number etc over a range of input rows.\n",
        "\n",
        "- It operates on a group of rows (like frame, partition) and return a single value for every input row."
      ],
      "metadata": {
        "id": "6TymJnNYybbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame named df\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, rank, dense_rank, min, max, sum, avg, countDistinct,lag,lead\n",
        "\n",
        "Data = ((\"Kirti\", \"Sales\", 3000), \\\n",
        "    (\"Leo\", \"Sales\", 4600),  \\\n",
        "    (\"Neha\", \"Sales\", 4100),   \\\n",
        "    (\"Priya\", \"Finance\", 3000),  \\\n",
        "    (\"Pooja\", \"Sales\", 3000),    \\\n",
        "    (\"Anjali\", \"Finance\", 3300),  \\\n",
        "    (\"Kritika\", \"Finance\", 3900),    \\\n",
        "    (\"Megha\", \"Marketing\", 3000), \\\n",
        "    (\"Megha\", \"Marketing\", 2000),\\\n",
        "    (\"Shweta\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data = Data, schema = columns)\n",
        "\n",
        "print(\"ORIGINAL DF\")\n",
        "df.show()\n",
        "\n",
        "# Create a window specification\n",
        "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "print(\"Using window function with order by\")\n",
        "\n",
        "# Example: Calculate row_number\n",
        "# Returns a sequential number starting from 1 within a window partition\n",
        "print(\"ROW NUMBER\")\n",
        "df.withColumn(\"row_number\", row_number().over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate rank\n",
        "# Returns the rank of rows within a window partition.\n",
        "# This function leaves gaps in rank when there are ties.\n",
        "print(\"RANK\")\n",
        "df.withColumn(\"rank\", rank().over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate dense_rank\n",
        "# Returns the rank of rows within a window partition without any gaps.\n",
        "# Where as Rank() returns rank with gaps.\n",
        "print(\"DENSE RANK\")\n",
        "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate sum\n",
        "print(\"SUM COLUMN\")\n",
        "df.withColumn(\"sum_column\", sum(\"salary\").over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate min\n",
        "print(\"MIN COLUMN\")\n",
        "df.withColumn(\"min_column\", min(\"salary\").over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate max\n",
        "print(\"MAX COLUMN\")\n",
        "df.withColumn(\"max_column\", max(\"salary\").over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate average\n",
        "print(\"AVG COLUMN\")\n",
        "df.withColumn(\"avg_column\", avg(\"salary\").over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate lag\n",
        "'''\n",
        "lead() function where we access subsequent rows, but in lag function,\n",
        "we access previous rows. It is a useful function in comparing\n",
        "the current row value from the previous row value.\n",
        "'''\n",
        "print(\"LAG\")\n",
        "df.withColumn(\"lag\",lag(\"salary\",1,default=0).over(windowSpec)).show()\n",
        "\n",
        "# Example: Calculate lead\n",
        "print(\"LEAD\")\n",
        "df.withColumn(\"lead\",lead(\"salary\",1,default=0).over(windowSpec)).show()\n",
        "\n",
        "# # Example: Calculate distinct count\n",
        "# print(\"DISTINCT COUNT\")\n",
        "# df = df.withColumn(\"distinct_count\", countDistinct(\"employee_name\").over(windowSpec)).show()\n",
        "\n",
        "# Show the DataFrame\n",
        "# df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BauLnYnrx_mC",
        "outputId": "b3e8c8ea-633d-4eea-dc17-2793930e4291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL DF\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        Kirti|     Sales|  3000|\n",
            "|          Leo|     Sales|  4600|\n",
            "|         Neha|     Sales|  4100|\n",
            "|        Priya|   Finance|  3000|\n",
            "|        Pooja|     Sales|  3000|\n",
            "|       Anjali|   Finance|  3300|\n",
            "|      Kritika|   Finance|  3900|\n",
            "|        Megha| Marketing|  3000|\n",
            "|        Megha| Marketing|  2000|\n",
            "|       Shweta|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n",
            "Using window function with order by\n",
            "ROW NUMBER\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|         1|\n",
            "|       Anjali|   Finance|  3300|         2|\n",
            "|      Kritika|   Finance|  3900|         3|\n",
            "|        Megha| Marketing|  2000|         1|\n",
            "|        Megha| Marketing|  3000|         2|\n",
            "|        Kirti|     Sales|  3000|         1|\n",
            "|        Pooja|     Sales|  3000|         2|\n",
            "|         Neha|     Sales|  4100|         3|\n",
            "|       Shweta|     Sales|  4100|         4|\n",
            "|          Leo|     Sales|  4600|         5|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "RANK\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Priya|   Finance|  3000|   1|\n",
            "|       Anjali|   Finance|  3300|   2|\n",
            "|      Kritika|   Finance|  3900|   3|\n",
            "|        Megha| Marketing|  2000|   1|\n",
            "|        Megha| Marketing|  3000|   2|\n",
            "|        Kirti|     Sales|  3000|   1|\n",
            "|        Pooja|     Sales|  3000|   1|\n",
            "|         Neha|     Sales|  4100|   3|\n",
            "|       Shweta|     Sales|  4100|   3|\n",
            "|          Leo|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "DENSE RANK\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|         1|\n",
            "|       Anjali|   Finance|  3300|         2|\n",
            "|      Kritika|   Finance|  3900|         3|\n",
            "|        Megha| Marketing|  2000|         1|\n",
            "|        Megha| Marketing|  3000|         2|\n",
            "|        Kirti|     Sales|  3000|         1|\n",
            "|        Pooja|     Sales|  3000|         1|\n",
            "|         Neha|     Sales|  4100|         2|\n",
            "|       Shweta|     Sales|  4100|         2|\n",
            "|          Leo|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "SUM COLUMN\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|sum_column|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|      3000|\n",
            "|       Anjali|   Finance|  3300|      6300|\n",
            "|      Kritika|   Finance|  3900|     10200|\n",
            "|        Megha| Marketing|  2000|      2000|\n",
            "|        Megha| Marketing|  3000|      5000|\n",
            "|        Kirti|     Sales|  3000|      6000|\n",
            "|        Pooja|     Sales|  3000|      6000|\n",
            "|         Neha|     Sales|  4100|     14200|\n",
            "|       Shweta|     Sales|  4100|     14200|\n",
            "|          Leo|     Sales|  4600|     18800|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "MIN COLUMN\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|min_column|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|      3000|\n",
            "|       Anjali|   Finance|  3300|      3000|\n",
            "|      Kritika|   Finance|  3900|      3000|\n",
            "|        Megha| Marketing|  2000|      2000|\n",
            "|        Megha| Marketing|  3000|      2000|\n",
            "|        Kirti|     Sales|  3000|      3000|\n",
            "|        Pooja|     Sales|  3000|      3000|\n",
            "|         Neha|     Sales|  4100|      3000|\n",
            "|       Shweta|     Sales|  4100|      3000|\n",
            "|          Leo|     Sales|  4600|      3000|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "MAX COLUMN\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|max_column|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|      3000|\n",
            "|       Anjali|   Finance|  3300|      3300|\n",
            "|      Kritika|   Finance|  3900|      3900|\n",
            "|        Megha| Marketing|  2000|      2000|\n",
            "|        Megha| Marketing|  3000|      3000|\n",
            "|        Kirti|     Sales|  3000|      3000|\n",
            "|        Pooja|     Sales|  3000|      3000|\n",
            "|         Neha|     Sales|  4100|      4100|\n",
            "|       Shweta|     Sales|  4100|      4100|\n",
            "|          Leo|     Sales|  4600|      4600|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "AVG COLUMN\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|avg_column|\n",
            "+-------------+----------+------+----------+\n",
            "|        Priya|   Finance|  3000|    3000.0|\n",
            "|       Anjali|   Finance|  3300|    3150.0|\n",
            "|      Kritika|   Finance|  3900|    3400.0|\n",
            "|        Megha| Marketing|  2000|    2000.0|\n",
            "|        Megha| Marketing|  3000|    2500.0|\n",
            "|        Kirti|     Sales|  3000|    3000.0|\n",
            "|        Pooja|     Sales|  3000|    3000.0|\n",
            "|         Neha|     Sales|  4100|    3550.0|\n",
            "|       Shweta|     Sales|  4100|    3550.0|\n",
            "|          Leo|     Sales|  4600|    3760.0|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "LAG\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary| lag|\n",
            "+-------------+----------+------+----+\n",
            "|        Priya|   Finance|  3000|   0|\n",
            "|       Anjali|   Finance|  3300|3000|\n",
            "|      Kritika|   Finance|  3900|3300|\n",
            "|        Megha| Marketing|  2000|   0|\n",
            "|        Megha| Marketing|  3000|2000|\n",
            "|        Kirti|     Sales|  3000|   0|\n",
            "|        Pooja|     Sales|  3000|3000|\n",
            "|         Neha|     Sales|  4100|3000|\n",
            "|       Shweta|     Sales|  4100|4100|\n",
            "|          Leo|     Sales|  4600|4100|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "LEAD\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|        Priya|   Finance|  3000|3300|\n",
            "|       Anjali|   Finance|  3300|3900|\n",
            "|      Kritika|   Finance|  3900|   0|\n",
            "|        Megha| Marketing|  2000|3000|\n",
            "|        Megha| Marketing|  3000|   0|\n",
            "|        Kirti|     Sales|  3000|3000|\n",
            "|        Pooja|     Sales|  3000|4100|\n",
            "|         Neha|     Sales|  4100|4100|\n",
            "|       Shweta|     Sales|  4100|4600|\n",
            "|          Leo|     Sales|  4600|   0|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAY 7"
      ],
      "metadata": {
        "id": "8y1GNkcQHpL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pyspark and MSSQL connect(connecting to azure db):"
      ],
      "metadata": {
        "id": "rCyrn5c8Hszl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MSSQL JDBC Connection\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"/content/drive/MyDrive/Colab Notebooks/mssql-jdbc-12.2.0.jre8.jar\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "kkIdpmEYH6to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a888be-0d40-4345-8a17-a03a598fb39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectionInfo:\n",
        "    JDBC_PARAM = \"jdbc\"\n",
        "    SQL_SERVER_DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "    SQL_URL=\"jdbc:sqlserver://4.240.11.180:1433;databaseName=Parijat;user=SparkTraining;password=SparkTraining@123;encrypt=true;trustServerCertificate=true\"\n",
        "\n",
        "\n",
        "query = \"(SELECT * from [Student_incremental]) as data\"\n",
        "viewDF = spark.read.format(ConnectionInfo.JDBC_PARAM).options(url=ConnectionInfo.SQL_URL,dbtable=query,driver=ConnectionInfo.SQL_SERVER_DRIVER).load()\n",
        "viewDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH9gcG1RUk5o",
        "outputId": "d324f446-a162-46ee-a0ad-85fb7f841671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+-------------------+-----------+---+\n",
            "| ID|            Name|             InDtTm|       Date|Age|\n",
            "+---+----------------+-------------------+-----------+---+\n",
            "|  1|   ABHISHEK 1234|2023-05-16 12:08:47|2023-03-30 | 26|\n",
            "|  2|   AMIT kumar 32|2023-05-29 13:51:46|2023-03-31 | 24|\n",
            "|  3|naveen gulati 32|2023-05-29 13:51:46|2023-03-01 | 25|\n",
            "|  4|            hary|2023-05-16 15:19:26|2023-03-02 | 27|\n",
            "|  5|           mansu|2023-05-16 12:44:08|2023-03-03 | 28|\n",
            "|  6|           sagar|2023-05-15 14:15:28|2023-03-04 | 21|\n",
            "|  7|          satyam|2023-05-15 14:15:28|2023-06-05 | 22|\n",
            "|  8|           ayush|2023-05-15 14:15:28|2023-06-06 | 27|\n",
            "|  9|          akshay|2023-05-15 14:15:28|2023-06-07 | 28|\n",
            "| 10|           dhruv|2023-05-15 14:15:28|2023-06-08 | 29|\n",
            "| 11|          ankush|2023-05-15 14:15:28|2023-06-09 | 20|\n",
            "| 12|         abhinav|2023-05-15 14:15:28|2023-06-10 | 23|\n",
            "| 13|          mayank|2023-05-15 14:15:28|2023-03-29 | 19|\n",
            "| 14|           hruva|2023-05-15 14:15:28|2023-03-28 | 17|\n",
            "| 15|            Maya|2023-05-15 14:15:28|2023-03-27 | 23|\n",
            "| 16|           navin|2023-05-15 14:15:28|2023-06-01 | 25|\n",
            "| 17|              BA|2023-05-15 14:15:28|2023-03-04 | 25|\n",
            "| 18|             ABC|2023-05-15 14:15:28|2023-06-11 | 15|\n",
            "| 19|             BCA|2023-05-15 14:15:28|2023-05-11 | 15|\n",
            "| 28|    abhinav test|2023-05-15 14:15:28|2023-05-10 | 28|\n",
            "+---+----------------+-------------------+-----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Incremental Task**\n",
        "\n",
        "1. Create a seperate notebook and give suitable name. Identify the table and columns for partitioning. Let's assume you have a table named \"student_incremental\" with a date column named \"Date\". Do full load of the data and create partitions on a monthly basis using (month and year from Date column).\n",
        "\n",
        "2. Check whether the parquet files for the last two months (if the current date is June 2023, the last 2 months would be May 2023 and June 2023 till today)exist or not. if exist then overwrite the incremental data only.\n",
        "\n",
        "3. Observe it if the incremental has performed completely or not. Also make sure that your code should be dynamic.\n",
        "\n",
        "\n",
        "4. Share the link of the assignment on https://forms.gle/nMmUDtxeNzWHvxxe7\n"
      ],
      "metadata": {
        "id": "gk-r_8zV-d4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK_gocr6VZzh",
        "outputId": "d8c1280f-2662-4f89-91a3-97fe5ef47688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Solution shared by one of the trainee. It can be optimized more. Approach may vary. '''\n",
        "\n",
        "!pip install pymssql\n",
        "\n",
        "from pyspark import SparkContext, SparkConf, SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "import sys,pymssql\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "# ------------------------------------exception handling ------------------------------------------------------------------------#\n",
        "try:\n",
        "    # ----------------------------creating the database connection--------------------------------------------------------------------#\n",
        "\n",
        "    spark = SparkSession.builder \\\n",
        "      .appName(\"PostgreSQL JDBC Connection\") \\\n",
        "      .config(\"spark.driver.extraClassPath\", \"/content/drive/MyDrive/Colab Notebooks/mssql-jdbc-12.2.0.jre8.jar\") \\\n",
        "      .getOrCreate()\n",
        "    # connection information to sql server\n",
        "    class ConnectionInfo:\n",
        "      JDBC_PARAM = \"jdbc\"\n",
        "      SQL_SERVER_DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "      SQL_URL=\"jdbc:sqlserver://4.240.11.180:1433;databaseName=Parijat;user=SparkTraining;password=SparkTraining@123;encrypt=true;trustServerCertificate=true\"\n",
        "\n",
        "        # -------------------Getting the list of tables stored in database parijat and making the connection with SQL server-------------------------------------------------------#\n",
        "\n",
        "    server=\"4.240.11.180\"\n",
        "    port=\"1433\"\n",
        "    database=\"Parijat\"\n",
        "    user=\"SparkTraining\"\n",
        "    password=\"SparkTraining@123\"\n",
        "    conn = pymssql.connect(server=server,port=port, user=user, password=password, database=database)\n",
        "\n",
        "   '''PUT YOUR LOGIC OF CODE HERE'''\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "        exception_type, exception_object, exception_traceback = sys.exc_info()\n",
        "        filename = exception_traceback.tb_frame.f_code.co_filename\n",
        "        line_number = exception_traceback.tb_lineno\n",
        "        print(\"Exception type: \", exception_type)\n",
        "        print(\"File name: \", filename)\n",
        "        print(\"Line number: \", line_number)"
      ],
      "metadata": {
        "id": "bP_FfJ3TVh6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 8"
      ],
      "metadata": {
        "id": "5vr1LZJfeKCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snapshotting**"
      ],
      "metadata": {
        "id": "CrTKHPbAeNVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Snapshotting in PySpark depends on the specific requirements and data sources.\n",
        "\n",
        "- The general steps involve loading the data into a DataFrame, applying a filter or transformation to create a snapshot, and then performing the desired analysis or operations on the snapshot DataFrame.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Dvi_RwQgYa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Month-end snapshot of an AR(Account Receivables) module in PySpark:**\n"
      ],
      "metadata": {
        "id": "p4QIbg9OVxyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a SparkSession.\n",
        "- Loads the AR transactions data from the source file into a DataFrame.\n",
        "- Extracts the year and month from the transaction date using the year and month functions.\n",
        "- Groups the transactions by customer, year, and month and calculates the month-end balance using the groupBy and agg functions.\n",
        "- Uses the last_day function to determine the month-end date and aliases it as \"snapshot_date\" or \"link_date\".\n",
        "- Calculates the sum of the transaction amounts and aliases it as \"balance\".\n",
        "- Saves the resulting AR snapshot DataFrame to a target location, such as a csv file  using the write.csv function.\n",
        "- Stop the SparkSession.\n"
      ],
      "metadata": {
        "id": "f9DpB4eEV3Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, month, last_day, sum\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder .appName(\"ARModuleSnapshot\").getOrCreate()\n",
        "\n",
        "# Load AR transactions data from source)\n",
        "arTransactions = spark.read.csv(\"path/to/ar_transactions.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Extract year and month from the transaction date\n",
        "arTransactions = arTransactions.withColumn(\"year\", year(col(\"transaction_date\")))\n",
        "arTransactions = arTransactions.withColumn(\"month\", month(col(\"transaction_date\")))\n",
        "\n",
        "# Group by customer, year, and month and calculate the month-end balance\n",
        "arSnapshot = arTransactions.groupBy(\"customer_id\", \"year\", \"month\") \\\n",
        "    .agg(last_day(max(\"transaction_date\")).alias(\"snapshot_date\"), sum(\"amount\").alias(\"balance\"))\n",
        "\n",
        "# Save the snapshot to a target location (e.g., a CSV file or database table)\n",
        "arSnapshot.write.csv(\"path/to/ar_snapshot.csv\", header=True)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "ovtEHZhFV-8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Month-end snapshot of an inventory module in PySpark:\n"
      ],
      "metadata": {
        "id": "YgHRQiUjWC6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max, date_format\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder .appName(\"InventorySnapshot\") .getOrCreate()\n",
        "\n",
        "# Load inventory data from CSV file\n",
        "inventory_df = spark.read.csv(\"inventory_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Convert date column to DateType\n",
        "inventory_df = inventory_df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
        "\n",
        "# Find the maximum date in the dataset\n",
        "max_date = inventory_df.select(max(\"date\")).collect()[0][0]\n",
        "\n",
        "# Filter the data for the maximum date of each month\n",
        "snapshot_df = inventory_df \\\n",
        "    .withColumn(\"year_month\", date_format(\"date\", \"yyyy-MM\")) \\\n",
        "    .withColumn(\"month_last_date\", date_format(max_date, \"yyyy-MM-dd\")) \\\n",
        "    .filter(col(\"date\") == col(\"month_last_date\"))\n",
        "\n",
        "# Create a window specification for partitioning by year and month\n",
        "window_spec = Window.partitionBy(\"year_month\")\n",
        "\n",
        "# Add a row number column within each partition\n",
        "snapshot_df = snapshot_df.withColumn(\"row_number\", row_number().over(window_spec))\n",
        "\n",
        "# Save the snapshot data to Parquet file\n",
        "snapshot_df.write.mode(\"overwrite\").parquet(\"inventory_snapshot.parquet\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "yuTHjUUhUsAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snapshotting Task**\n",
        "\n",
        "- Given the input file with columns 'Date', 'Branch_code', 'lot_code', 'net_qty' and output file with the same columns but date values are different:\n",
        "\n",
        "- You need to prepare a script on snapshotting of the input data on the basis of Date column where you'll group the data on the basis of 'Branch_code' and 'lot_code' and calculate the sum of 'net_qty' for all the dates from minimum to maximum date of that particular lot_code.\n",
        "\n",
        "- Make sure to check all the datatypes and do proper type casting.\n"
      ],
      "metadata": {
        "id": "poYFz0XjPDdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWo-Ga9WGCVi",
        "outputId": "4849e278-92ac-4bad-e2b2-7d7b29d4e9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Solution shared by one of the trainee. It can be optimized more. Approach may vary. '''\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from datetime import date, timedelta\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "# from _datetime import date\n",
        "from typing import Final\n",
        "from pyspark  import SparkContext ,SparkConf\n",
        "from pyspark.sql import Window\n",
        "import sys\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Snapshot_Task\").getOrCreate()\n",
        "\n",
        "inventory = spark.read.csv(path = r\"/content/drive/MyDrive/Colab Notebooks/Snapshot Input.csv\",sep = ',',header = True,quote = \"\")\n",
        "\n",
        "inventory = inventory.withColumn(\"Date\", to_date(\"Date\", 'dd-MMM-yy'))\n",
        "inventory = inventory.withColumn(\"net_qty\", col(\"net_qty\").cast(IntegerType()))\n",
        "inventory = inventory.withColumn(\"lot_code\", col(\"lot_code\").cast(IntegerType()))\n",
        "\n",
        "'''PUT YOUR LOGIC OF CODE HERE '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbdpa8TS4ROB",
        "outputId": "865c21d1-3dc8-46bc-ca44-6a029075e702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+--------+------+\n",
            "|      Date|Branch Code|lot code|Cumsum|\n",
            "+----------+-----------+--------+------+\n",
            "|2020-03-31|       5013| 4499295|     6|\n",
            "|2020-04-01|       5013| 4499295|     6|\n",
            "|2020-04-02|       5013| 4499295|     6|\n",
            "|2020-04-03|       5013| 4499295|     6|\n",
            "|2020-04-04|       5013| 4499295|     6|\n",
            "|2020-04-05|       5013| 4499295|     6|\n",
            "|2020-04-06|       5013| 4499295|     6|\n",
            "|2020-04-07|       5013| 4499295|     6|\n",
            "|2020-04-08|       5013| 4499295|     6|\n",
            "|2020-04-09|       5013| 4499295|     6|\n",
            "|2020-04-10|       5013| 4499295|     6|\n",
            "|2020-04-11|       5013| 4499295|     6|\n",
            "|2020-04-12|       5013| 4499295|     6|\n",
            "|2020-04-13|       5013| 4499295|     6|\n",
            "|2020-04-14|       5013| 4499295|     6|\n",
            "|2020-04-15|       5013| 4499295|     6|\n",
            "|2020-04-16|       5013| 4499295|     6|\n",
            "|2020-04-17|       5013| 4499295|     6|\n",
            "|2020-04-18|       5013| 4499295|     6|\n",
            "|2020-04-19|       5013| 4499295|     6|\n",
            "|2020-04-20|       5013| 4499295|     6|\n",
            "|2020-04-21|       5013| 4499295|     6|\n",
            "|2020-04-22|       5013| 4499295|     6|\n",
            "|2020-04-23|       5013| 4499295|     6|\n",
            "|2020-04-24|       5013| 4499295|     6|\n",
            "|2020-04-25|       5013| 4499295|     6|\n",
            "|2020-04-26|       5013| 4499295|     6|\n",
            "|2020-04-27|       5013| 4499295|     6|\n",
            "|2020-04-28|       5013| 4499295|     6|\n",
            "|2020-04-29|       5013| 4499295|     6|\n",
            "|2020-04-30|       5013| 4499295|     6|\n",
            "|2020-05-01|       5013| 4499295|     6|\n",
            "|2020-05-02|       5013| 4499295|     6|\n",
            "|2020-05-03|       5013| 4499295|     6|\n",
            "|2020-05-04|       5013| 4499295|     6|\n",
            "|2020-05-05|       5013| 4499295|     6|\n",
            "|2020-05-06|       5013| 4499295|     6|\n",
            "|2020-05-07|       5013| 4499295|     6|\n",
            "|2020-05-08|       5013| 4499295|     6|\n",
            "|2020-05-09|       5013| 4499295|     6|\n",
            "|2020-05-10|       5013| 4499295|     6|\n",
            "|2020-05-11|       5013| 4499295|     6|\n",
            "|2020-05-12|       5013| 4499295|     6|\n",
            "|2020-05-13|       5013| 4499295|     6|\n",
            "|2020-05-14|       5013| 4499295|     6|\n",
            "|2020-05-15|       5013| 4499295|     6|\n",
            "|2020-05-16|       5013| 4499295|     6|\n",
            "|2020-05-17|       5013| 4499295|     6|\n",
            "|2020-05-18|       5013| 4499295|     6|\n",
            "|2020-05-19|       5013| 4499295|     6|\n",
            "|2020-05-20|       5013| 4499295|     6|\n",
            "|2020-05-21|       5013| 4499295|     6|\n",
            "|2020-05-22|       5013| 4499295|     6|\n",
            "|2020-05-23|       5013| 4499295|     6|\n",
            "|2020-05-24|       5013| 4499295|     6|\n",
            "|2020-05-25|       5013| 4499295|     6|\n",
            "|2020-05-26|       5013| 4499295|     6|\n",
            "|2020-05-27|       5013| 4499295|     6|\n",
            "|2020-05-28|       5013| 4499295|     6|\n",
            "|2020-05-29|       5013| 4499295|     6|\n",
            "|2020-05-30|       5013| 4499295|     6|\n",
            "|2020-05-31|       5013| 4499295|     6|\n",
            "|2020-06-01|       5013| 4499295|     6|\n",
            "|2020-06-02|       5013| 4499295|     6|\n",
            "|2020-06-03|       5013| 4499295|     6|\n",
            "|2020-06-04|       5013| 4499295|     6|\n",
            "|2020-06-05|       5013| 4499295|     6|\n",
            "|2020-06-06|       5013| 4499295|     6|\n",
            "|2020-06-07|       5013| 4499295|     6|\n",
            "|2020-06-08|       5013| 4499295|     6|\n",
            "|2020-06-09|       5013| 4499295|     6|\n",
            "|2020-06-10|       5013| 4499295|     6|\n",
            "|2020-06-11|       5013| 4499295|     6|\n",
            "|2020-06-12|       5013| 4499295|     6|\n",
            "|2020-06-13|       5013| 4499295|     6|\n",
            "|2020-06-14|       5013| 4499295|     6|\n",
            "|2020-06-15|       5013| 4499295|     4|\n",
            "|2020-06-16|       5013| 4499295|     4|\n",
            "|2020-06-17|       5013| 4499295|     4|\n",
            "|2020-06-18|       5013| 4499295|     4|\n",
            "|2020-06-19|       5013| 4499295|     4|\n",
            "|2020-06-20|       5013| 4499295|     4|\n",
            "|2020-06-21|       5013| 4499295|     4|\n",
            "|2020-06-22|       5013| 4499295|     4|\n",
            "|2020-06-23|       5013| 4499295|     4|\n",
            "|2020-06-24|       5013| 4499295|     4|\n",
            "|2020-06-25|       5013| 4499295|     4|\n",
            "|2020-06-26|       5013| 4499295|     4|\n",
            "|2020-06-27|       5013| 4499295|     4|\n",
            "|2020-06-28|       5013| 4499295|     4|\n",
            "|2020-06-29|       5013| 4499295|     4|\n",
            "|2020-06-30|       5013| 4499295|     4|\n",
            "|2020-07-01|       5013| 4499295|     4|\n",
            "|2020-07-02|       5013| 4499295|     4|\n",
            "|2020-07-03|       5013| 4499295|     4|\n",
            "|2020-07-04|       5013| 4499295|     4|\n",
            "|2020-07-05|       5013| 4499295|     4|\n",
            "|2020-07-06|       5013| 4499295|     4|\n",
            "|2020-07-07|       5013| 4499295|     4|\n",
            "|2020-07-08|       5013| 4499295|     4|\n",
            "+----------+-----------+--------+------+\n",
            "only showing top 100 rows\n",
            "\n",
            "6944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RFC Connection**"
      ],
      "metadata": {
        "id": "F0mi5QWNQLQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyrfc import Connection\n",
        "from sqlalchemy import create_engine\n",
        "import datetime\n",
        "from logging import _startTime\n",
        "\n",
        "# Production\n",
        "ASHOST='*****'\n",
        "# CLIENT='**'\n",
        "SYSNR='**'\n",
        "System_Id = '***'\n",
        "USER='*****'\n",
        "PASSWD='******'\n",
        "\n",
        "conn = Connection(ashost=ASHOST, sysnr=SYSNR, user=USER, passwd=PASSWD,system=System_Id)\n",
        "\n",
        "# fetching response of the RFC\n",
        "response = conn.call('ZRFC_CUSTOMER_GRP_MASTER')\n",
        "df = pd.DataFrame(response['ET_DATA']) # table name\n",
        "print(df)\n",
        "\n",
        "'''*****************************************************************************'''\n",
        "# dumpping the data into postgresSQL\n",
        "engine = create_engine(f\"postgresql://url\")\n",
        "df.to_sql(\"customer_group_master\", engine,schema='schema_name',if_exists='replace',index=False)\n",
        "print(\"Data dumped\")\n",
        "\n",
        "'''*****************************************************************************'''\n",
        "\n",
        "#checking for particular values of a parameter and creating df then saving it to hdfs, reading it and dumping it to postgresSQL\n",
        "yr_ls=['2018','2019','2020','2021','2022','2023']\n",
        "for i in yr_ls:\n",
        "    response = conn.call('ZBH_RFC_BUDG_PLAN_KOCK',FYEAR=str(i))\n",
        "    print(response)\n",
        "    df = pd.DataFrame(response['IT_BUDGET'])\n",
        "    print(\"YEAR IS \", i)\n",
        "    print(df)\n",
        "    df = spark.createDataFrame(df.astype(str))\n",
        "    df.write.mode(\"append\").save(\"hdfs_path\")\n",
        "    df = spark.read.parquet(\"hdfs_path\")\n",
        "    properties = {\"user\": \"postgres\",\"password\": \"password\",\"driver\": \"org.postgresql.Driver\"}\n",
        "    df.write.jdbc(url = \"jdbc:postgresql://url\", table=\"pricol.Budget_Plan_Kockpit\", mode=\"append\", properties=properties)\n",
        "print('done')\n",
        "\n"
      ],
      "metadata": {
        "id": "ehj7f091QOpR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}